{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1024\n",
    "VOCAB_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE)\n",
    "        self.k_weight = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.q_weight = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.v_weight = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.output_weight = nn.Linear(HIDDEN_SIZE, VOCAB_SIZE)\n",
    "        self._div = np.sqrt(HIDDEN_SIZE)\n",
    "\n",
    "    def forward(self, x, past=None):\n",
    "        # x: B*L*1\n",
    "        if past is not None:\n",
    "            x = self.embedding(x[:,-1:])\n",
    "            q = self.q_weight(x)\n",
    "            k = self.k_weight(x)\n",
    "            v = self.v_weight(x)\n",
    "            past_k, past_v = past\n",
    "            # q = torch.cat((past_q, q), dim=1)\n",
    "            k = torch.cat((past_k, k), dim=1)\n",
    "            v = torch.cat((past_v, v), dim=1)\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            q = self.q_weight(x[:,-1:,:])\n",
    "            k = self.k_weight(x)\n",
    "            v = self.v_weight(x)\n",
    "        score = torch.softmax(torch.bmm(q, k.permute(0,2,1))/self._div, -1) # B*L*L\n",
    "        output = self.output_weight(torch.bmm(score, v)) # B*L*D\n",
    "        output_idx = torch.argmax(output, dim=-1)\n",
    "        return output_idx[:,-1:], (k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttentionDecoder(\n",
       "  (embedding): Embedding(1024, 1024)\n",
       "  (k_weight): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (q_weight): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (v_weight): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (output_weight): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = SelfAttentionDecoder()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([634, 634, 634, 634, 634, 634, 634, 634, 634, 634, 124, 634, 124, 634,\n",
      "        124, 634, 124, 634, 124, 634, 124, 634, 124, 634, 634, 124, 634, 124,\n",
      "        634, 124, 634, 124])\n",
      "tensor([634, 634, 634, 634, 634, 634, 634, 634, 634, 634, 124, 634, 124, 634,\n",
      "        124, 634, 124, 634, 124, 634, 124, 634, 124, 634, 634, 124, 634, 124,\n",
      "        634, 124, 634, 124])\n",
      "tensor([634, 634, 634, 634, 634, 634, 634, 634, 634, 634, 124, 634, 124, 634,\n",
      "        124, 634, 124, 634, 124, 634, 124, 634, 124, 634, 634, 124, 634, 124,\n",
      "        634, 124, 634, 124])\n",
      "tensor([634, 634, 634, 634, 634, 634, 634, 634, 634, 634, 124, 634, 124, 634,\n",
      "        124, 634, 124, 634, 124, 634, 124, 634, 124, 634, 634, 124, 634, 124,\n",
      "        634, 124, 634, 124])\n",
      "tensor([634, 634, 634, 634, 634, 634, 634, 634, 634, 634, 124, 634, 124, 634,\n",
      "        124, 634, 124, 634, 124, 634, 124, 634, 124, 634, 634, 124, 634, 124,\n",
      "        634, 124, 634, 124])\n",
      "334 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 1\n",
    "sample = torch.LongTensor([list(range(512))]*5)\n",
    "MASK = torch.LongTensor([[0]]*5)\n",
    "with torch.no_grad():\n",
    "    for step in range(32):\n",
    "        # assume we generate 32 tokens\n",
    "        # assume 0 is [MASK]\n",
    "        output_idx, _ = decoder(torch.cat((sample, MASK), dim=1))\n",
    "        sample = torch.cat([sample, output_idx], dim=-1)\n",
    "print(sample[0][512:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([634, 634, 634, 185, 185, 124, 124, 124, 124, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664])\n",
      "tensor([634, 634, 634, 185, 185, 124, 124, 124, 124, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664])\n",
      "tensor([634, 634, 634, 185, 185, 124, 124, 124, 124, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664])\n",
      "tensor([634, 634, 634, 185, 185, 124, 124, 124, 124, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664])\n",
      "tensor([634, 634, 634, 185, 185, 124, 124, 124, 124, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664, 664,\n",
      "        664, 664, 664, 664])\n",
      "69.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 1\n",
    "sample = torch.LongTensor([list(range(512))]*5)\n",
    "MASK = torch.LongTensor([[0]]*5)\n",
    "past = None\n",
    "with torch.no_grad():\n",
    "    for step in range(32):\n",
    "        # assume we generate 32 tokens\n",
    "        # assume 0 is [MASK]\n",
    "        output_idx, past = decoder(torch.cat((sample, MASK), dim=1), past=past)\n",
    "        sample = torch.cat([sample, output_idx], dim=-1)\n",
    "print(sample[0][512:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.rand((10, 512, HIDDEN_SIZE), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.6 ms ± 1.73 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "for _ in range(3):\n",
    "    linear(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = torch.rand((10, 1, HIDDEN_SIZE), dtype=torch.float)\n",
    "cache = torch.rand((10, 511, HIDDEN_SIZE), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.77 ms ± 93.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "for _ in range(3):\n",
    "    o = linear(sample2)\n",
    "    torch.cat((cache, o), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448b48b023bcc9c3d4a79e814720a10ca6d4244f75e0f7ce4af58f96ba2b7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
