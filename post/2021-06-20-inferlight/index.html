<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Impvove Inference Efficiency with Batch Inference | 多头注意力</title>
<meta name="keywords" content="Inference">
<meta name="description" content="As an algorithm engineer, it is inevitable that you will encounter the problem of bringing models online in your daily work. For some less demanding scenarios, you can handle this by utilizing a web framework: for each user request, call the model to infer and return the result. However, this straightforward implementation often fails to maximize the use of the GPU, and is slightly overwhelming for scenarios with high performance requirements.">
<meta name="author" content="">
<link rel="canonical" href="https://www.yuanhao.site/post/2021-06-20-inferlight/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.yuanhao.site/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuanhao.site/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuanhao.site/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.yuanhao.site/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.yuanhao.site/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Impvove Inference Efficiency with Batch Inference" />
<meta property="og:description" content="As an algorithm engineer, it is inevitable that you will encounter the problem of bringing models online in your daily work. For some less demanding scenarios, you can handle this by utilizing a web framework: for each user request, call the model to infer and return the result. However, this straightforward implementation often fails to maximize the use of the GPU, and is slightly overwhelming for scenarios with high performance requirements." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.yuanhao.site/post/2021-06-20-inferlight/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-06-20T10:25:03+00:00" />
<meta property="article:modified_time" content="2021-06-20T10:25:03+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Impvove Inference Efficiency with Batch Inference"/>
<meta name="twitter:description" content="As an algorithm engineer, it is inevitable that you will encounter the problem of bringing models online in your daily work. For some less demanding scenarios, you can handle this by utilizing a web framework: for each user request, call the model to infer and return the result. However, this straightforward implementation often fails to maximize the use of the GPU, and is slightly overwhelming for scenarios with high performance requirements."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://www.yuanhao.site/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Impvove Inference Efficiency with Batch Inference",
      "item": "https://www.yuanhao.site/post/2021-06-20-inferlight/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Impvove Inference Efficiency with Batch Inference",
  "name": "Impvove Inference Efficiency with Batch Inference",
  "description": "As an algorithm engineer, it is inevitable that you will encounter the problem of bringing models online in your daily work. For some less demanding scenarios, you can handle this by utilizing a web framework: for each user request, call the model to infer and return the result. However, this straightforward implementation often fails to maximize the use of the GPU, and is slightly overwhelming for scenarios with high performance requirements.",
  "keywords": [
    "Inference"
  ],
  "articleBody": "As an algorithm engineer, it is inevitable that you will encounter the problem of bringing models online in your daily work. For some less demanding scenarios, you can handle this by utilizing a web framework: for each user request, call the model to infer and return the result. However, this straightforward implementation often fails to maximize the use of the GPU, and is slightly overwhelming for scenarios with high performance requirements.\nThere are many ways to optimize, and one useful tip is to change from inference for each request to inference for multiple requests at once. Last year, about this time I wrote a small tool to achieve this function and gave it a rather overbearing name InferLight. Honestly, that tool was not very well implemented. Recently, I refactor the tool with reference to Shannon Technology’s Service-Streamer .\nThis feature seems simple, but in the process of implementation, we can understand a lot of Python asynchronous programming knowledge and feel the parallel computing power of modern GPU.\nArchitecture First, to improve the model’s online inference throughput, you should make the inference service asynchronous. For web services, asynchronous means that the program can handle other requests while the model is computing. For Python, asynchronous services can be implemented with good Asyncio-based frameworks, such as Sanic , which I commonly use. Whereas inference is computationally intensive, our goal is to be able to aggregate multiple inference requests, make efficient use of the parallel computing power of the GPU, and be able to return the results of bulk inference to the corresponding requestor correctly.\nTo achieve the above goal, the following modules are needed\nFront-end service: used to receive requests and return results. It can be various protocols such as Http, PRC, etc. It is an independent process. Inference Worker: responsible for model initialization, bulk inference data construction, and inference calculation. It is an independent process. Task queue: the front-end service receives the request and sends the calculation task to the task queue; the inference worker listens to the queue and takes out a small batch each time by the model inference Result queue: After the inference done, inference worker sends the result to the result queue; the front-end service listens to the queue and gets the inference result Result distribution: before sending the task to the task queue, a unique identifier of the task needs to be generated, and the result corresponding to the task is obtained according to the identifier after retrieving the result from the result queue There are many ways to implement the task queue and result queue, and you can use some mature middleware such as Kafka and Redis. To avoid external dependencies, I chose to use Python’s native multi-process queue this time. The result queue is listened to and distributed through a sub-thread of the front-end service process.\nImplementation The inference worker is relatively simple. Since there are a variety of models to load and data processing steps, I designed the inference worker as a base class that is inherited and implements specific methods when used.\nclass BaseInferLightWorker: def __init__(self, data_queue:mp.Queue, result_queue:mp.Queue, model_args:dict, batch_size=16, max_delay=0.1, ready_event=None) -\u003e None: self.data_queue = data_queue self.result_queue = result_queue self.batch_size = batch_size self.max_delay = max_delay self.logger = logging.getLogger('InferLight-Worker') self.logger.setLevel(logging.DEBUG) self.load_model(model_args) # Inform parent process when model loaded if ready_event: ready_event.set() def run(self): self.logger.info('Worker started!') while True: data, task_ids = [], [] since = time.time() for i in range(self.batch_size): try: # get data form data queue d = self.data_queue.get(block=True, timeout=self.max_delay) task_ids.append(d[0]) data.append(d[1]) self.logger.info('get one new task') except Empty: pass if time.time()-since\u003e=self.max_delay: break if len(data)\u003e0: start = time.perf_counter() batch = self.build_batch(data) results = self.inference(batch) end = time.perf_counter() time_elapsed = (end-start)*1000 self.logger.info(f'inference succeeded. batch size: {len(data)}, time elapsed: {time_elapsed:.3f} ms') # write results to result queue for (task_id, result) in zip(task_ids, results): self.result_queue.put((task_id, result)) def build_batch(self, requests): raise NotImplementedError def inference(self, batch): raise NotImplementedError def load_model(self, model_args): raise NotImplementedError @classmethod def start(cls, data_queue:mp.Queue, result_queue:mp.Queue, model_args:dict, batch_size=16, max_delay=0.1,ready_event=None): w = cls(data_queue, result_queue, model_args, batch_size, max_delay, ready_event) w.run() Along with this is a Wrapper class used in the front-end service to do the request receiving, result collection and distribution of inference requests.\nimport asyncio import logging import multiprocessing as mp import threading import uuid from queue import Empty from cachetools import TTLCache from .data import InferStatus, InferResponse class LightWrapper: def __init__(self, worker_class, model_args: dict, batch_size=16, max_delay=0.1) -\u003e None: # setup logger self.logger = logging.getLogger('InferLight-Wrapper') self.logger.setLevel(logging.INFO) # save results in a TTL cache self.result_cache = TTLCache(maxsize=10000, ttl=5) self.mp = mp.get_context('spawn') self.result_queue = self.mp.Queue() self.data_queue = self.mp.Queue() # start inference worker process self.logger.info('Starting worker...') worker_ready_event = self.mp.Event() self._worker_p = self.mp.Process(target=worker_class.start, args=( self.data_queue, self.result_queue, model_args, batch_size, max_delay, worker_ready_event ), daemon=True) self._worker_p.start() # wait at most 30 seconds is_ready = worker_ready_event.wait(timeout=30) if is_ready: self.logger.info('Worker started!') else: self.logger.error('Failed to start worker!') # start the result collecting thread self.back_thread = threading.Thread( target=self._collect_result, name=\"thread_collect_result\") self.back_thread.daemon = True self.back_thread.start() def _collect_result(self): # keep reading result queue # write result to cache with task_id as key self.logger.info('Result collecting thread started!') while True: try: msg = self.result_queue.get(block=True, timeout=0.01) except Empty: msg = None if msg is not None: (task_id, result) = msg self.result_cache[task_id] = result async def get_result(self, task_id): # non-blocking check result while task_id not in self.result_cache: await asyncio.sleep(0.01) return self.result_cache[task_id] async def predict(self, input, timeout=2) -\u003e InferResponse: # generate unique task_id task_id = str(uuid.uuid4()) # send input to worker process self.data_queue.put((task_id, input)) try: # here we set a timeout threshold to avoid waiting forever result = await asyncio.wait_for(self.get_result(task_id), timeout=timeout) except asyncio.TimeoutError: return InferResponse(InferStatus.TIMEOUT, None) return InferResponse(InferStatus.SUCCEED, result) Some of the data structures used are defined as follows\nfrom enum import Enum class InferStatus(Enum): SUCCEED = 0 TIMEOUT = 1 class InferResponse: def __init__(self, status: InferStatus, result) -\u003e None: self.status = status self.result = result def succeed(self): return self.status==InferStatus.SUCCEED Use Case and Test Result Here we show how the above components can be used with a sentiment analysis BERT model.\nFirst define the model\nclass BertModel(nn.Module): def __init__(self, config) -\u003e None: super().__init__() self.config = config self.bert = AutoModelForSequenceClassification.from_pretrained(config['model']) self.bert.eval() self.device = torch.device('cuda' if config.get('use_cuda') else 'cpu') self.bert.to(self.device) def forward(self, inputs): return self.bert(**inputs).logits Then inherit BaseInferLightWorker and implement three functions to get a complete Worker class\nclass MyWorker(BaseInferLightWorker): def load_model(self, model_args): self.model = BertModel(model_args) self.tokenizer = AutoTokenizer.from_pretrained(model_args['model']) self.device = torch.device('cuda' if model_args.get('use_cuda') else 'cpu') return def build_batch(self, requests): # 这个函数用来构建batch inference的输入 encoded_input = self.tokenizer.batch_encode_plus(requests, return_tensors='pt', padding=True, truncation=True, max_length=512) return encoded_input.to(self.device) @torch.no_grad() def inference(self, batch): model_output = self.model.forward(batch).cpu().numpy() scores = softmax(model_output, axis=1) # 将整个batch的结果以list形式返回即可 ret = [x.tolist() for x in scores] return ret Finally, building services\nif __name__=='__main__': # for convenience，we use a fixed text from Aesop's Fables as input text = \"\"\" A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree. The grapes seemed ready to burst with juice, and the Fox's mouth watered as he gazed longingly at them. The bunch hung from a high branch, and the Fox had to jump for it. The first time he jumped he missed it by a long way. So he walked off a short distance and took a running leap at it, only to fall short once more. Again and again he tried, but in vain. Now he sat down and looked at the grapes in disgust. \"What a fool I am,\" he said. \"Here I am wearing myself out to get a bunch of sour grapes that are not worth gaping for.\" And off he walked very, very scornfully. \"\"\" config = { 'model':\"nlptown/bert-base-multilingual-uncased-sentiment\", 'use_cuda':True } wrapped_model = LightWrapper(MyWorker, config, batch_size=16, max_delay=0.05) app = Sanic('test') @app.get('/batch_predict') async def batched_predict(request): dummy_input = text response = await wrapped_model.predict(dummy_input) if not response.succeed(): return json_response({'output':None, 'status':'failed'}) return json_response({'output': response.result}) app.run(port=8888) I did some tests with the famous Apache’s ab tool. I started the above app on my HP Z4 Workstation and made sure the worker process was running on a RTX 6000 GPU.\nWith ab -n 1000 -c 32 http://localhost:8888/batched_predict, I got the following result.\nConcurrency Level: 32 Time taken for tests: 4.019 seconds Complete requests: 1000 Failed requests: 999 (Connect: 0, Receive: 0, Length: 999, Exceptions: 0) Total transferred: 202978 bytes HTML transferred: 111978 bytes Requests per second: 248.79 [#/sec] (mean) Time per request: 128.620 [ms] (mean) Time per request: 4.019 [ms] (mean, across all concurrent requests) Transfer rate: 49.32 [Kbytes/sec] received Test result of another straightford implement without batch inference is as follow:\nConcurrency Level: 32 Time taken for tests: 10.164 seconds Complete requests: 1000 Failed requests: 0 Total transferred: 202000 bytes HTML transferred: 111000 bytes Requests per second: 98.39 [#/sec] (mean) Time per request: 325.234 [ms] (mean) Time per request: 10.164 [ms] (mean, across all concurrent requests) Transfer rate: 19.41 [Kbytes/sec] received As you can see, we got about 2.5 times throughput with batch inference! When doing the benchmark, I also observed that the GPU utilization is much higher with batch inference.\nI have opened source the InferLight, and it can be found at https://github.com/thuwyh/InferLight. Hope you love it :)\n",
  "wordCount" : "1485",
  "inLanguage": "en",
  "datePublished": "2021-06-20T10:25:03Z",
  "dateModified": "2021-06-20T10:25:03Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.yuanhao.site/post/2021-06-20-inferlight/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "多头注意力",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.yuanhao.site/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.yuanhao.site" accesskey="h" title="多头注意力 (Alt + H)">多头注意力</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.yuanhao.site/archive" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://www.yuanhao.site">Home</a>&nbsp;»&nbsp;<a href="https://www.yuanhao.site/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Impvove Inference Efficiency with Batch Inference
    </h1>
    <div class="post-meta"><span title='2021-06-20 10:25:03 +0000 UTC'>June 20, 2021</span>&nbsp;·&nbsp;7 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#architecture" aria-label="Architecture">Architecture</a></li>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a></li>
                <li>
                    <a href="#use-case-and-test-result" aria-label="Use Case and Test Result">Use Case and Test Result</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>As an algorithm engineer, it is inevitable that you will encounter the problem of bringing models online in your daily work. For some less demanding scenarios, you can handle this by utilizing a web framework: for each user request, call the model to infer and return the result. However, this straightforward implementation often fails to maximize the use of the GPU, and is slightly overwhelming for scenarios with high performance requirements.</p>
<p>There are many ways to optimize, and one useful tip is to change from inference for each request to inference for multiple requests at once. Last year, about this time I wrote a small tool to achieve this function and gave it a rather overbearing name InferLight. Honestly,  that tool was not very well implemented. Recently, I refactor the tool with reference to Shannon Technology&rsquo;s Service-Streamer .</p>
<p>This feature seems simple, but in the process of implementation, we can understand a lot of Python asynchronous programming knowledge and feel the parallel computing power of modern GPU.</p>
<h2 id="architecture">Architecture<a hidden class="anchor" aria-hidden="true" href="#architecture">#</a></h2>
<p>First, to improve the model&rsquo;s online inference throughput, you should make the inference service asynchronous. For web services, asynchronous means that the program can handle other requests while the model is computing. For Python, asynchronous services can be implemented with good Asyncio-based frameworks, such as Sanic , which I commonly use. Whereas inference is computationally intensive, our goal is to be able to aggregate multiple inference requests, make efficient use of the parallel computing power of the GPU, and be able to return the results of bulk inference to the corresponding requestor correctly.</p>
<p>To achieve the above goal, the following modules are needed</p>
<ol>
<li>Front-end service: used to receive requests and return results. It can be various protocols such as Http, PRC, etc. It is an independent process.</li>
<li>Inference Worker: responsible for model initialization, bulk inference data construction, and inference calculation. It is an independent process.</li>
<li>Task queue: the front-end service receives the request and sends the calculation task to the task queue; the inference worker listens to the queue and takes out a small batch each time by the model inference</li>
<li>Result queue: After the inference done, inference worker sends the result to the result queue; the front-end service listens to the queue and gets the inference result</li>
<li>Result distribution: before sending the task to the task queue, a unique identifier of the task needs to be generated, and the result corresponding to the task is obtained according to the identifier after retrieving the result from the result queue</li>
</ol>
<p>There are many ways to implement the task queue and result queue, and you can use some mature middleware such as Kafka and Redis. To avoid external dependencies, I chose to use Python&rsquo;s native multi-process queue this time. The result queue is listened to and distributed through a sub-thread of the front-end service process.</p>
<h2 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h2>
<p>The inference worker is relatively simple. Since there are a variety of models to load and data processing steps, I designed the inference worker as a base class that is inherited and implements specific methods when used.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BaseInferLightWorker</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, data_queue:mp<span style="color:#f92672">.</span>Queue, result_queue:mp<span style="color:#f92672">.</span>Queue, 
</span></span><span style="display:flex;"><span>                 model_args:dict, 
</span></span><span style="display:flex;"><span>                 batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, max_delay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>                 ready_event<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data_queue <span style="color:#f92672">=</span> data_queue
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>result_queue <span style="color:#f92672">=</span> result_queue
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">=</span> batch_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>max_delay <span style="color:#f92672">=</span> max_delay
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logger <span style="color:#f92672">=</span> logging<span style="color:#f92672">.</span>getLogger(<span style="color:#e6db74">&#39;InferLight-Worker&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>setLevel(logging<span style="color:#f92672">.</span>DEBUG)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>load_model(model_args)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Inform parent process when model loaded</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> ready_event:
</span></span><span style="display:flex;"><span>            ready_event<span style="color:#f92672">.</span>set()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>info(<span style="color:#e6db74">&#39;Worker started!&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>            data, task_ids <span style="color:#f92672">=</span> [], []
</span></span><span style="display:flex;"><span>            since <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>batch_size):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>                    <span style="color:#75715e"># get data form data queue</span>
</span></span><span style="display:flex;"><span>                    d <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>data_queue<span style="color:#f92672">.</span>get(block<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, timeout<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>max_delay)
</span></span><span style="display:flex;"><span>                    task_ids<span style="color:#f92672">.</span>append(d[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>                    data<span style="color:#f92672">.</span>append(d[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>                    self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>info(<span style="color:#e6db74">&#39;get one new task&#39;</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">except</span> Empty:
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> time<span style="color:#f92672">.</span>time()<span style="color:#f92672">-</span>since<span style="color:#f92672">&gt;=</span>self<span style="color:#f92672">.</span>max_delay:
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> len(data)<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>                batch <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>build_batch(data)
</span></span><span style="display:flex;"><span>                results <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>inference(batch)
</span></span><span style="display:flex;"><span>                end <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>                time_elapsed <span style="color:#f92672">=</span> (end<span style="color:#f92672">-</span>start)<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>info(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;inference succeeded. batch size: </span><span style="color:#e6db74">{</span>len(data)<span style="color:#e6db74">}</span><span style="color:#e6db74">, time elapsed: </span><span style="color:#e6db74">{</span>time_elapsed<span style="color:#e6db74">:</span><span style="color:#e6db74">.3f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms&#39;</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># write results to result queue</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> (task_id, result) <span style="color:#f92672">in</span> zip(task_ids, results):
</span></span><span style="display:flex;"><span>                    self<span style="color:#f92672">.</span>result_queue<span style="color:#f92672">.</span>put((task_id, result))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_batch</span>(self, requests):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">NotImplementedError</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">inference</span>(self, batch):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">NotImplementedError</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_model</span>(self, model_args):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">NotImplementedError</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@classmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">start</span>(cls, data_queue:mp<span style="color:#f92672">.</span>Queue, result_queue:mp<span style="color:#f92672">.</span>Queue, model_args:dict, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, max_delay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,ready_event<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        w <span style="color:#f92672">=</span> cls(data_queue, result_queue, model_args, batch_size, max_delay, ready_event)
</span></span><span style="display:flex;"><span>        w<span style="color:#f92672">.</span>run()
</span></span></code></pre></div><p>Along with this is a Wrapper class used in the front-end service to do the request receiving, result collection and distribution of inference requests.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> asyncio
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> logging
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> multiprocessing <span style="color:#66d9ef">as</span> mp
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> threading
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> uuid
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> queue <span style="color:#f92672">import</span> Empty
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> cachetools <span style="color:#f92672">import</span> TTLCache
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> .data <span style="color:#f92672">import</span> InferStatus, InferResponse
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LightWrapper</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, worker_class, model_args: dict,
</span></span><span style="display:flex;"><span>                 batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, max_delay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># setup logger</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logger <span style="color:#f92672">=</span> logging<span style="color:#f92672">.</span>getLogger(<span style="color:#e6db74">&#39;InferLight-Wrapper&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>setLevel(logging<span style="color:#f92672">.</span>INFO)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># save results in a TTL cache</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>result_cache <span style="color:#f92672">=</span> TTLCache(maxsize<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>, ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mp <span style="color:#f92672">=</span> mp<span style="color:#f92672">.</span>get_context(<span style="color:#e6db74">&#39;spawn&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>result_queue <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mp<span style="color:#f92672">.</span>Queue()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data_queue <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mp<span style="color:#f92672">.</span>Queue()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># start inference worker process</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>info(<span style="color:#e6db74">&#39;Starting worker...&#39;</span>)
</span></span><span style="display:flex;"><span>        worker_ready_event <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mp<span style="color:#f92672">.</span>Event()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_worker_p <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mp<span style="color:#f92672">.</span>Process(target<span style="color:#f92672">=</span>worker_class<span style="color:#f92672">.</span>start, args<span style="color:#f92672">=</span>(
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>data_queue, self<span style="color:#f92672">.</span>result_queue, model_args, batch_size, max_delay, worker_ready_event
</span></span><span style="display:flex;"><span>        ), daemon<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_worker_p<span style="color:#f92672">.</span>start()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># wait at most 30 seconds</span>
</span></span><span style="display:flex;"><span>        is_ready <span style="color:#f92672">=</span> worker_ready_event<span style="color:#f92672">.</span>wait(timeout<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> is_ready:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>info(<span style="color:#e6db74">&#39;Worker started!&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>error(<span style="color:#e6db74">&#39;Failed to start worker!&#39;</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># start the result collecting thread</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>back_thread <span style="color:#f92672">=</span> threading<span style="color:#f92672">.</span>Thread(
</span></span><span style="display:flex;"><span>            target<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_collect_result, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;thread_collect_result&#34;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>back_thread<span style="color:#f92672">.</span>daemon <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>back_thread<span style="color:#f92672">.</span>start()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_collect_result</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># keep reading result queue</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># write result to cache with task_id as key</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logger<span style="color:#f92672">.</span>info(<span style="color:#e6db74">&#39;Result collecting thread started!&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>                msg <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>result_queue<span style="color:#f92672">.</span>get(block<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, timeout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">except</span> Empty:
</span></span><span style="display:flex;"><span>                msg <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> msg <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                (task_id, result) <span style="color:#f92672">=</span> msg
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>result_cache[task_id] <span style="color:#f92672">=</span> result
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_result</span>(self, task_id):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># non-blocking check result</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> task_id <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>result_cache:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">await</span> asyncio<span style="color:#f92672">.</span>sleep(<span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>result_cache[task_id]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, input, timeout<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>) <span style="color:#f92672">-&gt;</span> InferResponse:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># generate unique task_id</span>
</span></span><span style="display:flex;"><span>        task_id <span style="color:#f92672">=</span> str(uuid<span style="color:#f92672">.</span>uuid4())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># send input to worker process</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data_queue<span style="color:#f92672">.</span>put((task_id, input))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># here we set a timeout threshold to avoid waiting forever</span>
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> asyncio<span style="color:#f92672">.</span>wait_for(self<span style="color:#f92672">.</span>get_result(task_id), timeout<span style="color:#f92672">=</span>timeout)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">except</span> asyncio<span style="color:#f92672">.</span>TimeoutError:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> InferResponse(InferStatus<span style="color:#f92672">.</span>TIMEOUT, <span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> InferResponse(InferStatus<span style="color:#f92672">.</span>SUCCEED, result)
</span></span></code></pre></div><p>Some of the data structures used are defined as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> enum <span style="color:#f92672">import</span> Enum
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">InferStatus</span>(Enum):
</span></span><span style="display:flex;"><span>  SUCCEED <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>  TIMEOUT <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">InferResponse</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> __init__(self, status: InferStatus, result) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>status <span style="color:#f92672">=</span> status
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>result <span style="color:#f92672">=</span> result
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">succeed</span>(self):
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>status<span style="color:#f92672">==</span>InferStatus<span style="color:#f92672">.</span>SUCCEED
</span></span></code></pre></div><h2 id="use-case-and-test-result">Use Case and Test Result<a hidden class="anchor" aria-hidden="true" href="#use-case-and-test-result">#</a></h2>
<p>Here we show how the above components can be used with a sentiment analysis BERT model.</p>
<p>First define the model</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BertModel</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> AutoModelForSequenceClassification<span style="color:#f92672">.</span>from_pretrained(config[<span style="color:#e6db74">&#39;model&#39;</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bert<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> config<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;use_cuda&#39;</span>) <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;cpu&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bert<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, inputs):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>bert(<span style="color:#f92672">**</span>inputs)<span style="color:#f92672">.</span>logits
</span></span></code></pre></div><p>Then inherit BaseInferLightWorker and implement three functions to get a complete Worker class</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MyWorker</span>(BaseInferLightWorker):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_model</span>(self, model_args):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> BertModel(model_args)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_args[<span style="color:#e6db74">&#39;model&#39;</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> model_args<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;use_cuda&#39;</span>) <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;cpu&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_batch</span>(self, requests):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 这个函数用来构建batch inference的输入</span>
</span></span><span style="display:flex;"><span>        encoded_input <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>tokenizer<span style="color:#f92672">.</span>batch_encode_plus(requests, 
</span></span><span style="display:flex;"><span>                                                         return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pt&#39;</span>,
</span></span><span style="display:flex;"><span>                                                         padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                                                         truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                                                         max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> encoded_input<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@torch.no_grad</span>()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">inference</span>(self, batch):
</span></span><span style="display:flex;"><span>        model_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>forward(batch)<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> softmax(model_output, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 将整个batch的结果以list形式返回即可</span>
</span></span><span style="display:flex;"><span>        ret <span style="color:#f92672">=</span> [x<span style="color:#f92672">.</span>tolist() <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> scores]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> ret
</span></span></code></pre></div><p>Finally, building services</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># for convenience，we use a fixed text from Aesop&#39;s Fables as input</span>
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree. The grapes seemed ready to burst with juice, and the Fox&#39;s mouth watered as he gazed longingly at them.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The bunch hung from a high branch, and the Fox had to jump for it. The first time he jumped he missed it by a long way. So he walked off a short distance and took a running leap at it, only to fall short once more. Again and again he tried, but in vain.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Now he sat down and looked at the grapes in disgust.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;What a fool I am,&#34; he said. &#34;Here I am wearing myself out to get a bunch of sour grapes that are not worth gaping for.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    And off he walked very, very scornfully.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    config <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;model&#39;</span>:<span style="color:#e6db74">&#34;nlptown/bert-base-multilingual-uncased-sentiment&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;use_cuda&#39;</span>:<span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    wrapped_model <span style="color:#f92672">=</span> LightWrapper(MyWorker, config, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, max_delay<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    app <span style="color:#f92672">=</span> Sanic(<span style="color:#e6db74">&#39;test&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@app.get</span>(<span style="color:#e6db74">&#39;/batch_predict&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batched_predict</span>(request):
</span></span><span style="display:flex;"><span>        dummy_input <span style="color:#f92672">=</span> text
</span></span><span style="display:flex;"><span>        response <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> wrapped_model<span style="color:#f92672">.</span>predict(dummy_input)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> response<span style="color:#f92672">.</span>succeed():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> json_response({<span style="color:#e6db74">&#39;output&#39;</span>:<span style="color:#66d9ef">None</span>, <span style="color:#e6db74">&#39;status&#39;</span>:<span style="color:#e6db74">&#39;failed&#39;</span>})
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> json_response({<span style="color:#e6db74">&#39;output&#39;</span>: response<span style="color:#f92672">.</span>result})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    app<span style="color:#f92672">.</span>run(port<span style="color:#f92672">=</span><span style="color:#ae81ff">8888</span>)
</span></span></code></pre></div><p>I did some tests with the famous Apache’s ab tool. I started the above app on my HP Z4 Workstation and made sure the worker process was running on a RTX 6000 GPU.</p>
<p>With <code>ab -n 1000 -c 32 http://localhost:8888/batched_predict</code>, I got the following result.</p>
<pre tabindex="0"><code>Concurrency Level:      32
Time taken for tests:   4.019 seconds
Complete requests:      1000
Failed requests:        999
   (Connect: 0, Receive: 0, Length: 999, Exceptions: 0)
Total transferred:      202978 bytes
HTML transferred:       111978 bytes
Requests per second:    248.79 [#/sec] (mean)
Time per request:       128.620 [ms] (mean)
Time per request:       4.019 [ms] (mean, across all concurrent requests)
Transfer rate:          49.32 [Kbytes/sec] received
</code></pre><p>Test result of another straightford implement without batch inference is as follow:</p>
<pre tabindex="0"><code>Concurrency Level:      32
Time taken for tests:   10.164 seconds
Complete requests:      1000
Failed requests:        0
Total transferred:      202000 bytes
HTML transferred:       111000 bytes
Requests per second:    98.39 [#/sec] (mean)
Time per request:       325.234 [ms] (mean)
Time per request:       10.164 [ms] (mean, across all concurrent requests)
Transfer rate:          19.41 [Kbytes/sec] received
</code></pre><p>As you can see, we got about 2.5 times throughput with batch inference! When doing the benchmark, I also observed that the GPU utilization is much higher with batch inference.</p>
<p>I have opened source the InferLight, and it can be found at <a href="https://github.com/thuwyh/InferLight">https://github.com/thuwyh/InferLight</a>. Hope you love it :)</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.yuanhao.site/tags/inference/">Inference</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://www.yuanhao.site/post/2021-09-21-%E5%A2%9E%E9%95%BF%E9%BB%91%E5%AE%A2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">
    <span class="title">« Prev</span>
    <br>
    <span>《增长黑客》阅读笔记</span>
  </a>
  <a class="next" href="https://www.yuanhao.site/post/2021-06-20-%E6%A8%A1%E5%9E%8B%E4%B8%8A%E7%BA%BF%E4%B8%8D%E7%94%A8%E6%84%81%E6%89%B9%E9%87%8F%E6%8E%A8%E7%90%86%E6%9D%A5%E5%8A%A0%E6%B2%B9/">
    <span class="title">Next »</span>
    <br>
    <span>模型上线不用愁，批量推理来加油.</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Impvove Inference Efficiency with Batch Inference on x"
            href="https://x.com/intent/tweet/?text=Impvove%20Inference%20Efficiency%20with%20Batch%20Inference&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f&amp;hashtags=Inference">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Impvove Inference Efficiency with Batch Inference on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f&amp;title=Impvove%20Inference%20Efficiency%20with%20Batch%20Inference&amp;summary=Impvove%20Inference%20Efficiency%20with%20Batch%20Inference&amp;source=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Impvove Inference Efficiency with Batch Inference on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f&title=Impvove%20Inference%20Efficiency%20with%20Batch%20Inference">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Impvove Inference Efficiency with Batch Inference on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Impvove Inference Efficiency with Batch Inference on whatsapp"
            href="https://api.whatsapp.com/send?text=Impvove%20Inference%20Efficiency%20with%20Batch%20Inference%20-%20https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Impvove Inference Efficiency with Batch Inference on telegram"
            href="https://telegram.me/share/url?text=Impvove%20Inference%20Efficiency%20with%20Batch%20Inference&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Impvove Inference Efficiency with Batch Inference on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Impvove%20Inference%20Efficiency%20with%20Batch%20Inference&u=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2021-06-20-inferlight%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://www.yuanhao.site">多头注意力</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
