<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>大规模语言模型的基石数据集 | 多头注意力</title>
<meta name="keywords" content="大模型, NLP, ChatGPT, 语言模型, 数据集, CommonCrawl, C4, ArXiv, Books, LLaMA">
<meta name="description" content="最近AI大火，作为一名稍微有点赶不上趟的NLP工程师，感觉有很多课需要补。恰好昨天Meta发了新的大模型论文，浏览了一下发现很适合作为补课的切入点。
今天这部分是关于预训练使用的数据集，是重中之重，说数据是当代AI的基石一点也不为过。GPT3用的数据其实没有公开，Meta这次论文里提到的应该算是开源模型里一个最全的版本。他们使用的数据如下表所示，我们一一来看一下。
Dataset Sampling prop. Epochs Disk size CommonCrawl 67.0% 1.10 3.3 TB C4 15.0% 1.06 783 GB Github 4.5% 0.64 328 GB Wikipedia 4.5% 2.45 83 GB Books 4.5% 2.23 85 GB ArXiv 2.5% 1.06 92 GB StackExchange 2.0% 1.03 78 GB CommonCrawl 占比最大的数据集，他们的网站是https://commoncrawl.org/。我感觉这真是一个伟大的项目，7年时间爬了超多的互联网网页，涵盖了40种语言。
CommonCrawl网站截图 {: .align-caption style=&ldquo;text-align:center;font-size:smaller&rdquo;}
根据他们博客的最新数据，2023年二月版的数据包含了400TB的数据（纯文本的数据是9个多tb），三十多亿个网页。
The crawl archive for January/February 2023 is now available! The data was crawled January 26 – February 9 and contains 3.">
<meta name="author" content="">
<link rel="canonical" href="https://www.yuanhao.site/post/deeplearning/2023-02-26-ai-dataset/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.yuanhao.site/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuanhao.site/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuanhao.site/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.yuanhao.site/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.yuanhao.site/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="大规模语言模型的基石数据集" />
<meta property="og:description" content="最近AI大火，作为一名稍微有点赶不上趟的NLP工程师，感觉有很多课需要补。恰好昨天Meta发了新的大模型论文，浏览了一下发现很适合作为补课的切入点。
今天这部分是关于预训练使用的数据集，是重中之重，说数据是当代AI的基石一点也不为过。GPT3用的数据其实没有公开，Meta这次论文里提到的应该算是开源模型里一个最全的版本。他们使用的数据如下表所示，我们一一来看一下。
Dataset Sampling prop. Epochs Disk size CommonCrawl 67.0% 1.10 3.3 TB C4 15.0% 1.06 783 GB Github 4.5% 0.64 328 GB Wikipedia 4.5% 2.45 83 GB Books 4.5% 2.23 85 GB ArXiv 2.5% 1.06 92 GB StackExchange 2.0% 1.03 78 GB CommonCrawl 占比最大的数据集，他们的网站是https://commoncrawl.org/。我感觉这真是一个伟大的项目，7年时间爬了超多的互联网网页，涵盖了40种语言。
CommonCrawl网站截图 {: .align-caption style=&ldquo;text-align:center;font-size:smaller&rdquo;}
根据他们博客的最新数据，2023年二月版的数据包含了400TB的数据（纯文本的数据是9个多tb），三十多亿个网页。
The crawl archive for January/February 2023 is now available! The data was crawled January 26 – February 9 and contains 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.yuanhao.site/post/deeplearning/2023-02-26-ai-dataset/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-02-26T10:25:03+00:00" />
<meta property="article:modified_time" content="2023-02-26T10:25:03+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="大规模语言模型的基石数据集"/>
<meta name="twitter:description" content="最近AI大火，作为一名稍微有点赶不上趟的NLP工程师，感觉有很多课需要补。恰好昨天Meta发了新的大模型论文，浏览了一下发现很适合作为补课的切入点。
今天这部分是关于预训练使用的数据集，是重中之重，说数据是当代AI的基石一点也不为过。GPT3用的数据其实没有公开，Meta这次论文里提到的应该算是开源模型里一个最全的版本。他们使用的数据如下表所示，我们一一来看一下。
Dataset Sampling prop. Epochs Disk size CommonCrawl 67.0% 1.10 3.3 TB C4 15.0% 1.06 783 GB Github 4.5% 0.64 328 GB Wikipedia 4.5% 2.45 83 GB Books 4.5% 2.23 85 GB ArXiv 2.5% 1.06 92 GB StackExchange 2.0% 1.03 78 GB CommonCrawl 占比最大的数据集，他们的网站是https://commoncrawl.org/。我感觉这真是一个伟大的项目，7年时间爬了超多的互联网网页，涵盖了40种语言。
CommonCrawl网站截图 {: .align-caption style=&ldquo;text-align:center;font-size:smaller&rdquo;}
根据他们博客的最新数据，2023年二月版的数据包含了400TB的数据（纯文本的数据是9个多tb），三十多亿个网页。
The crawl archive for January/February 2023 is now available! The data was crawled January 26 – February 9 and contains 3."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://www.yuanhao.site/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "大规模语言模型的基石数据集",
      "item": "https://www.yuanhao.site/post/deeplearning/2023-02-26-ai-dataset/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大规模语言模型的基石数据集",
  "name": "大规模语言模型的基石数据集",
  "description": "最近AI大火，作为一名稍微有点赶不上趟的NLP工程师，感觉有很多课需要补。恰好昨天Meta发了新的大模型论文，浏览了一下发现很适合作为补课的切入点。\n今天这部分是关于预训练使用的数据集，是重中之重，说数据是当代AI的基石一点也不为过。GPT3用的数据其实没有公开，Meta这次论文里提到的应该算是开源模型里一个最全的版本。他们使用的数据如下表所示，我们一一来看一下。\nDataset Sampling prop. Epochs Disk size CommonCrawl 67.0% 1.10 3.3 TB C4 15.0% 1.06 783 GB Github 4.5% 0.64 328 GB Wikipedia 4.5% 2.45 83 GB Books 4.5% 2.23 85 GB ArXiv 2.5% 1.06 92 GB StackExchange 2.0% 1.03 78 GB CommonCrawl 占比最大的数据集，他们的网站是https://commoncrawl.org/。我感觉这真是一个伟大的项目，7年时间爬了超多的互联网网页，涵盖了40种语言。\nCommonCrawl网站截图 {: .align-caption style=\u0026ldquo;text-align:center;font-size:smaller\u0026rdquo;}\n根据他们博客的最新数据，2023年二月版的数据包含了400TB的数据（纯文本的数据是9个多tb），三十多亿个网页。\nThe crawl archive for January/February 2023 is now available! The data was crawled January 26 – February 9 and contains 3.",
  "keywords": [
    "大模型", "NLP", "ChatGPT", "语言模型", "数据集", "CommonCrawl", "C4", "ArXiv", "Books", "LLaMA"
  ],
  "articleBody": "最近AI大火，作为一名稍微有点赶不上趟的NLP工程师，感觉有很多课需要补。恰好昨天Meta发了新的大模型论文，浏览了一下发现很适合作为补课的切入点。\n今天这部分是关于预训练使用的数据集，是重中之重，说数据是当代AI的基石一点也不为过。GPT3用的数据其实没有公开，Meta这次论文里提到的应该算是开源模型里一个最全的版本。他们使用的数据如下表所示，我们一一来看一下。\nDataset Sampling prop. Epochs Disk size CommonCrawl 67.0% 1.10 3.3 TB C4 15.0% 1.06 783 GB Github 4.5% 0.64 328 GB Wikipedia 4.5% 2.45 83 GB Books 4.5% 2.23 85 GB ArXiv 2.5% 1.06 92 GB StackExchange 2.0% 1.03 78 GB CommonCrawl 占比最大的数据集，他们的网站是https://commoncrawl.org/。我感觉这真是一个伟大的项目，7年时间爬了超多的互联网网页，涵盖了40种语言。\nCommonCrawl网站截图 {: .align-caption style=“text-align:center;font-size:smaller”}\n根据他们博客的最新数据，2023年二月版的数据包含了400TB的数据（纯文本的数据是9个多tb），三十多亿个网页。\nThe crawl archive for January/February 2023 is now available! The data was crawled January 26 – February 9 and contains 3.15 billion web pages or 400 TiB of uncompressed content. Page captures are from 40 million hosts or 33 million registered domains and include 1.3 billion new URLs, not visited in any of our prior crawls.\n而LLaMa里面CommonCrawl的数据只有3个多TB，大概是总数据量的三分之一。可见数据的后处理工作量是相当大的。\nWe preprocess five CommonCrawl dumps, ranging from 2017 to 2020, with the CCNet pipeline (Wenzek et al.,2020). This process deduplicates the data at the line level, performs language identification with a fastText linear classifier to remove non-English pages and filters low quality content with an ngram language model.\nC4 占比第二大的数据集是C4，他的全称是Colossal Clean Crawled Corpus（4个C，所以叫C4）。这个数据集是在CommonCrawl数据集的基础上后处理而来。\n根据C4官网的介绍，用500个worker处理CommonCrawl数据集得到C4数据集需要大约16个小时\nThe C4 dataset we created for unsupervised pre-training is available in TensorFlow Datasets, but it requires a significant amount of bandwidth for downloading the raw Common Crawl scrapes (~7 TB) and compute for its preparation (~335 CPU-days). We suggest you take advantage of the Apache Beam support in TFDS, which enables distributed preprocessing of the dataset and can be run on Google Cloud Dataflow. With 500 workers, the job should complete in ~16 hours.\nGithub 第三占比的是Github数据集，这个在多年以前的预训练语言模型例如BERT、GPT里几乎没有人用。之前似乎看过一种说法是代码数据的加入对语言模型的逻辑推理能力有极大的帮助。这个点后面计划专门花点时间学习。\nWikipedia 维基百科数据因为质量高、覆盖面广是预训练语言模型的常用语料了，多年之前大家就爱使用。和Books数据集一道基本是预训练语言模型的标配。这里有一个很有趣的数字是整个维基百科的数据量只有不到100GB，甚至比github上的代码还少，这可是人类很大一部分知识啊。\nDeberta论文里不同预训练模型使用数据的对比。所有模型和2023年的大模型比数据量都小了一个量级 {: .align-caption style=“text-align:center;font-size:smaller”}\nBooks 论文里的books数据集特指books3，这个数据集没有特别正式的官网，其介绍出现在一个github issue里。根据作者的介绍，它包含了约20万本书籍。\nbooks3.tar.gz (37GB), aka “all of bibliotik in plain .txt form”, aka 197,000 books processed in exactly the same way as I did for bookcorpus here. So basically 11x bigger.\n这个数据集也是社区共同努力的结果\nThis is possible thanks to two organizations. First and foremost, thank you to the-eye.eu. They have a wonderful community (see discord), and they are extremely interested in archiving data for the benefit of humanity. Secondly, thank you to “The Pile”, which is the project that has been meticulously gathering and preparing this training data. Join their discord if you’re interested in ML: https://www.eleuther.ai/get-involved\n略显讽刺的是，以Open命名的OpenAI却没有公开他们在论文里使用的Books2数据集。\nbooks3.tar.gz seems to be similar to OpenAI’s mysterious “books2” dataset referenced in their papers. Unfortunately OpenAI will not give details, so we know very little about any differences. People suspect it’s “all of libgen”, but it’s purely conjecture. Nonetheless, books3 is “all of bibliotik”, which is possibly useful to anyone doing NLP work.\nArXiv 这个数据集感觉也是最近几年才流行加到预训练语言模型里的。学术论文的逻辑性比较强，我估计这也和近年来模型的推理能力提升有密切的关系。\nStackExchange StackOverflow各位读者，特别是码农朋友可能更加熟悉，StackExchange可以理解为是它的超集。StackExchange包含有不限于计算机的各种各样不同领域的高质量问答。在LLaMA的训练数据里，Meta只保留了若干个子领域。\nWe kept the data from the 28 largest websites, removed the HTML tags from text and sorted the answers by score (from highest to lowest).\n小结 我感觉除了Books以外CommonCrawl应该包含了剩下的其他数据集，Meta在训练的时候还显示地加入它们，是否等价于调整了数据的权重让高质量的网络内容出现地更多一些？论文中在C4数据集处有提到一点原因，说是加入不同预处理的数据有助于模型提升。\nDuring exploratory experiments, we observed that using diverse pre-processed CommonCrawl datasets improves performance.\n从这个角度看，要训练一个高质量的基础模型真的是有很多细节需要掌握，不是简单地堆数据、堆算力就能搞定的。\n另外，我今天在微博上看到有人嘲讽说Meta一开源，国内的“自主创新”马上就要来了。但其实不难看出这个模型里中文语料的比例应该是很低的。首先最大头CommonCrawl只保留了英文，维基只保留了拉丁语系20种语言的内容，ArXiv和StackExchange上面本来就几乎没有中文。也就是说，中文基本只有可能比较大规模地出现在Books和Github这两块。如此说来，这个模型的中文水平应该不会好到哪里去，这个博主也有点为黑而黑的意思。\n{: .align-center style=“width:40%”} 国内模型将迎来共产主义？ {: .align-caption style=“text-align:center;font-size:smaller”}\n3年前GPT3的repo里有个按照语言统计的数据量，在文档维度，中文只占到了0.11631%。从这个角度，各位家长一定要坚持让孩子学好英文，即使将来人工智能真的到来了，最好的版本一定是用英文交互的。\n",
  "wordCount" : "447",
  "inLanguage": "en",
  "datePublished": "2023-02-26T10:25:03Z",
  "dateModified": "2023-02-26T10:25:03Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.yuanhao.site/post/deeplearning/2023-02-26-ai-dataset/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "多头注意力",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.yuanhao.site/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.yuanhao.site" accesskey="h" title="多头注意力 (Alt + H)">多头注意力</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.yuanhao.site/archive" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://www.yuanhao.site">Home</a>&nbsp;»&nbsp;<a href="https://www.yuanhao.site/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      大规模语言模型的基石数据集
    </h1>
    <div class="post-meta"><span title='2023-02-26 10:25:03 +0000 UTC'>February 26, 2023</span>&nbsp;·&nbsp;3 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#commoncrawl" aria-label="CommonCrawl">CommonCrawl</a></li>
                <li>
                    <a href="#c4" aria-label="C4">C4</a></li>
                <li>
                    <a href="#github" aria-label="Github">Github</a></li>
                <li>
                    <a href="#wikipedia" aria-label="Wikipedia">Wikipedia</a></li>
                <li>
                    <a href="#books" aria-label="Books">Books</a></li>
                <li>
                    <a href="#arxiv" aria-label="ArXiv">ArXiv</a></li>
                <li>
                    <a href="#stackexchange" aria-label="StackExchange">StackExchange</a></li>
                <li>
                    <a href="#%e5%b0%8f%e7%bb%93" aria-label="小结">小结</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>最近AI大火，作为一名稍微有点赶不上趟的NLP工程师，感觉有很多课需要补。恰好昨天<a href="https://scontent-xsp1-1.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=ov6yTHfLfNQAX8WM6j3&amp;_nc_ht=scontent-xsp1-1.xx&amp;oh=00_AfDQq_MRNvWE4p7Hz5MrPQzYHuoBvWDmv9LMuPByqlsJCA&amp;oe=63FFCFA2" title="LLaMA论文">Meta发了新的大模型论文</a>，浏览了一下发现很适合作为补课的切入点。</p>
<p>今天这部分是关于预训练使用的<strong>数据集</strong>，是重中之重，说数据是当代AI的基石一点也不为过。GPT3用的数据其实没有公开，Meta这次论文里提到的应该算是开源模型里一个最全的版本。他们使用的数据如下表所示，我们一一来看一下。</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Sampling prop.</th>
<th>Epochs</th>
<th>Disk size</th>
</tr>
</thead>
<tbody>
<tr>
<td>CommonCrawl</td>
<td>67.0%</td>
<td>1.10</td>
<td>3.3 TB</td>
</tr>
<tr>
<td>C4</td>
<td>15.0%</td>
<td>1.06</td>
<td>783 GB</td>
</tr>
<tr>
<td>Github</td>
<td>4.5%</td>
<td>0.64</td>
<td>328 GB</td>
</tr>
<tr>
<td>Wikipedia</td>
<td>4.5%</td>
<td>2.45</td>
<td>83 GB</td>
</tr>
<tr>
<td>Books</td>
<td>4.5%</td>
<td>2.23</td>
<td>85 GB</td>
</tr>
<tr>
<td>ArXiv</td>
<td>2.5%</td>
<td>1.06</td>
<td>92 GB</td>
</tr>
<tr>
<td>StackExchange</td>
<td>2.0%</td>
<td>1.03</td>
<td>78 GB</td>
</tr>
</tbody>
</table>
<h2 id="commoncrawl">CommonCrawl<a hidden class="anchor" aria-hidden="true" href="#commoncrawl">#</a></h2>
<p>占比最大的数据集，他们的网站是<a href="https://commoncrawl.org/">https://commoncrawl.org/</a>。我感觉这真是一个伟大的项目，7年时间爬了超多的互联网网页，涵盖了40种语言。</p>
<p><img loading="lazy" src="/assets/largelm/common_crawl.png" alt="CommonCrawl"  />

CommonCrawl网站截图
{: .align-caption style=&ldquo;text-align:center;font-size:smaller&rdquo;}</p>
<p>根据他们<a href="https://commoncrawl.org/connect/blog/">博客</a>的最新数据，2023年二月版的数据包含了400TB的数据（纯文本的数据是9个多tb），三十多亿个网页。</p>
<blockquote>
<p>The crawl archive for January/February 2023 is now available! The data was crawled January 26 – February 9 and contains 3.15 billion web pages or 400 TiB of uncompressed content. Page captures are from 40 million hosts or 33 million registered domains and include 1.3 billion new URLs, not visited in any of our prior crawls.</p>
</blockquote>
<p>而LLaMa里面CommonCrawl的数据只有3个多TB，大概是总数据量的三分之一。可见数据的后处理工作量是相当大的。</p>
<blockquote>
<p>We preprocess five CommonCrawl dumps, ranging from 2017 to 2020, with the <a href="https://github.com/facebookresearch/cc_net">CCNet</a> pipeline (Wenzek et al.,2020). This process deduplicates the data at the line level, performs language identification with a fastText linear classifier to remove non-English pages and filters low quality content with an ngram language model.</p>
</blockquote>
<h2 id="c4">C4<a hidden class="anchor" aria-hidden="true" href="#c4">#</a></h2>
<p>占比第二大的数据集是C4，他的全称是Colossal Clean Crawled Corpus（4个C，所以叫C4）。这个数据集是在CommonCrawl数据集的基础上后处理而来。</p>
<p>根据<a href="https://www.tensorflow.org/datasets/catalog/c4">C4官网</a>的介绍，用500个worker处理CommonCrawl数据集得到C4数据集需要大约16个小时</p>
<blockquote>
<p>The C4 dataset we created for unsupervised pre-training is available in TensorFlow Datasets, but it requires a significant amount of bandwidth for downloading the raw Common Crawl scrapes (~7 TB) and compute for its preparation (~335 CPU-days). We suggest you take advantage of the Apache Beam support in TFDS, which enables distributed preprocessing of the dataset and can be run on Google Cloud Dataflow. With 500 workers, the job should complete in ~16 hours.</p>
</blockquote>
<h2 id="github">Github<a hidden class="anchor" aria-hidden="true" href="#github">#</a></h2>
<p>第三占比的是Github数据集，这个在多年以前的预训练语言模型例如BERT、GPT里几乎没有人用。之前似乎看过一种说法是代码数据的加入对语言模型的逻辑推理能力有极大的帮助。这个点后面计划专门花点时间学习。</p>
<h2 id="wikipedia">Wikipedia<a hidden class="anchor" aria-hidden="true" href="#wikipedia">#</a></h2>
<p>维基百科数据因为质量高、覆盖面广是预训练语言模型的常用语料了，多年之前大家就爱使用。和Books数据集一道基本是预训练语言模型的标配。这里有一个很有趣的数字是整个维基百科的数据量只有不到100GB，甚至比github上的代码还少，这可是人类很大一部分知识啊。</p>
<p><img loading="lazy" src="/assets/largelm/deberta_data.png" alt="Deberta论文里不同预训练模型使用数据的对比"  />

Deberta论文里不同预训练模型使用数据的对比。所有模型和2023年的大模型比数据量都小了一个量级
{: .align-caption style=&ldquo;text-align:center;font-size:smaller&rdquo;}</p>
<h2 id="books">Books<a hidden class="anchor" aria-hidden="true" href="#books">#</a></h2>
<p>论文里的books数据集特指books3，这个数据集没有特别正式的官网，其介绍出现在一个<a href="https://github.com/soskek/bookcorpus/issues/27#issuecomment-716104208" title="Books3">github issue</a>里。根据作者的介绍，它包含了约20万本书籍。</p>
<blockquote>
<p>books3.tar.gz (37GB), aka &ldquo;all of bibliotik in plain .txt form&rdquo;, aka 197,000 books processed in exactly the same way as I did for bookcorpus here. So basically 11x bigger.</p>
</blockquote>
<p>这个数据集也是社区共同努力的结果</p>
<blockquote>
<p>This is possible thanks to two organizations. First and foremost, thank you to the-eye.eu. They have a wonderful community (see discord), and they are extremely interested in archiving data for the benefit of humanity.
Secondly, thank you to &ldquo;The Pile&rdquo;, which is the project that has been meticulously gathering and preparing this training data. Join their discord if you&rsquo;re interested in ML: <a href="https://www.eleuther.ai/get-involved">https://www.eleuther.ai/get-involved</a></p>
</blockquote>
<p>略显讽刺的是，以Open命名的OpenAI却没有公开他们在论文里使用的<code>Books2</code>数据集。</p>
<blockquote>
<p>books3.tar.gz seems to be similar to OpenAI&rsquo;s mysterious &ldquo;books2&rdquo; dataset referenced in their papers. Unfortunately OpenAI will not give details, so we know very little about any differences. People suspect it&rsquo;s &ldquo;all of libgen&rdquo;, but it&rsquo;s purely conjecture. Nonetheless, books3 is &ldquo;all of bibliotik&rdquo;, which is possibly useful to anyone doing NLP work.</p>
</blockquote>
<h2 id="arxiv">ArXiv<a hidden class="anchor" aria-hidden="true" href="#arxiv">#</a></h2>
<p>这个数据集感觉也是最近几年才流行加到预训练语言模型里的。学术论文的逻辑性比较强，我估计这也和近年来模型的推理能力提升有密切的关系。</p>
<h2 id="stackexchange">StackExchange<a hidden class="anchor" aria-hidden="true" href="#stackexchange">#</a></h2>
<p>StackOverflow各位读者，特别是码农朋友可能更加熟悉，StackExchange可以理解为是它的超集。StackExchange包含有不限于计算机的各种各样不同领域的高质量问答。在LLaMA的训练数据里，Meta只保留了若干个子领域。</p>
<blockquote>
<p>We kept the data from the 28 largest websites, removed the HTML tags from text and sorted the answers by score (from highest to lowest).</p>
</blockquote>
<h2 id="小结">小结<a hidden class="anchor" aria-hidden="true" href="#小结">#</a></h2>
<p>我感觉除了Books以外CommonCrawl应该包含了剩下的其他数据集，Meta在训练的时候还显示地加入它们，是否等价于调整了数据的权重让高质量的网络内容出现地更多一些？论文中在C4数据集处有提到一点原因，说是加入不同预处理的数据有助于模型提升。</p>
<blockquote>
<p>During exploratory experiments, we observed that using diverse pre-processed CommonCrawl datasets improves performance.</p>
</blockquote>
<p>从这个角度看，要训练一个高质量的基础模型真的是有很多细节需要掌握，不是简单地堆数据、堆算力就能搞定的。</p>
<p>另外，我今天在微博上看到有人嘲讽说Meta一开源，国内的“自主创新”马上就要来了。但其实不难看出这个模型里中文语料的比例应该是很低的。首先最大头CommonCrawl只保留了英文，维基只保留了拉丁语系20种语言的内容，ArXiv和StackExchange上面本来就几乎没有中文。也就是说，中文基本只有可能比较大规模地出现在Books和Github这两块。如此说来，这个模型的中文水平应该不会好到哪里去，这个博主也有点为黑而黑的意思。</p>
<p><img loading="lazy" src="/assets/largelm/weibo.jpeg" alt="国内模型将迎来共产主义？"  />
{: .align-center style=&ldquo;width:40%&rdquo;}
国内模型将迎来共产主义？
{: .align-caption style=&ldquo;text-align:center;font-size:smaller&rdquo;}</p>
<p>3年前<a href="https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_document_count.csv" title="GPT3 training data by language">GPT3的repo</a>里有个按照语言统计的数据量，在文档维度，中文只占到了<strong>0.11631%</strong>。从这个角度，各位家长一定要坚持让孩子学好英文，即使将来人工智能真的到来了，最好的版本一定是用英文交互的。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.yuanhao.site/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></li>
      <li><a href="https://www.yuanhao.site/tags/nlp/">NLP</a></li>
      <li><a href="https://www.yuanhao.site/tags/chatgpt/">ChatGPT</a></li>
      <li><a href="https://www.yuanhao.site/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">语言模型</a></li>
      <li><a href="https://www.yuanhao.site/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</a></li>
      <li><a href="https://www.yuanhao.site/tags/commoncrawl/">CommonCrawl</a></li>
      <li><a href="https://www.yuanhao.site/tags/c4/">C4</a></li>
      <li><a href="https://www.yuanhao.site/tags/arxiv/">ArXiv</a></li>
      <li><a href="https://www.yuanhao.site/tags/books/">Books</a></li>
      <li><a href="https://www.yuanhao.site/tags/llama/">LLaMA</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://www.yuanhao.site/post/deeplearning/2023-02-28-fortune-teller/">
    <span class="title">« Prev</span>
    <br>
    <span>如何使用ChatGPT算命</span>
  </a>
  <a class="next" href="https://www.yuanhao.site/post/2023-02-16-chatgpt9/">
    <span class="title">Next »</span>
    <br>
    <span>ChatGPT未来会拥有自我情感和思维吗？</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大规模语言模型的基石数据集 on x"
            href="https://x.com/intent/tweet/?text=%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3%e6%95%b0%e6%8d%ae%e9%9b%86&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f&amp;hashtags=%e5%a4%a7%e6%a8%a1%e5%9e%8b%2cNLP%2cChatGPT%2c%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%2c%e6%95%b0%e6%8d%ae%e9%9b%86%2cCommonCrawl%2cC4%2cArXiv%2cBooks%2cLLaMA">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大规模语言模型的基石数据集 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f&amp;title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3%e6%95%b0%e6%8d%ae%e9%9b%86&amp;summary=%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3%e6%95%b0%e6%8d%ae%e9%9b%86&amp;source=https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大规模语言模型的基石数据集 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f&title=%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3%e6%95%b0%e6%8d%ae%e9%9b%86">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大规模语言模型的基石数据集 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大规模语言模型的基石数据集 on whatsapp"
            href="https://api.whatsapp.com/send?text=%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3%e6%95%b0%e6%8d%ae%e9%9b%86%20-%20https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大规模语言模型的基石数据集 on telegram"
            href="https://telegram.me/share/url?text=%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3%e6%95%b0%e6%8d%ae%e9%9b%86&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大规模语言模型的基石数据集 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3%e6%95%b0%e6%8d%ae%e9%9b%86&u=https%3a%2f%2fwww.yuanhao.site%2fpost%2fdeeplearning%2f2023-02-26-ai-dataset%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://www.yuanhao.site">多头注意力</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
