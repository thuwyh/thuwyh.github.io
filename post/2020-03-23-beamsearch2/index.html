<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>十分钟读懂beam search-2 | 多头注意力</title>
<meta name="keywords" content="NLP, 文本生成, beam search">
<meta name="description" content="在上一篇文章中我们介绍了基础版的beam search，这篇文章是对它的一个扩展，可以在模型不改的情况下获得更好的生成结果。今天的介绍围绕的也是一篇蛮新的论文，《The Curious Case of Neural Text Degeneration》，根据这篇论文的版面内容，它应该已经被ICLR 2020接收了。
Beam Search的问题 先解释以下什么要对Beam Search进行改进。因为Beam Search虽然比贪心有所改进，但还是会生成出空洞、重复、前后矛盾的文本。如果你有文本生成经验，一定对这些现象并不陌生。在语言模型还不像如今的BERT、GPT这么厉害的时候，这种现象更加明显。
没有经验也没关系，我们来看一个论文里面的例子。输入模型的引文（context)
&ldquo;The study, published in the Proceedings of the They were cattle called Bolivian Cavalleros; they live in a National Academy of Sciences of the United States of remote desert uninterrupted by town, and they speak huge, America (PNAS), was conducted by researchers from the beautiful, paradisiacal Bolivian linguistic thing. They say, Universidad Nacional Autónoma de México (UNAM) and">
<meta name="author" content="">
<link rel="canonical" href="https://www.yuanhao.site/post/2020-03-23-beamsearch2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.yuanhao.site/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuanhao.site/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuanhao.site/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.yuanhao.site/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.yuanhao.site/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="十分钟读懂beam search-2" />
<meta property="og:description" content="在上一篇文章中我们介绍了基础版的beam search，这篇文章是对它的一个扩展，可以在模型不改的情况下获得更好的生成结果。今天的介绍围绕的也是一篇蛮新的论文，《The Curious Case of Neural Text Degeneration》，根据这篇论文的版面内容，它应该已经被ICLR 2020接收了。
Beam Search的问题 先解释以下什么要对Beam Search进行改进。因为Beam Search虽然比贪心有所改进，但还是会生成出空洞、重复、前后矛盾的文本。如果你有文本生成经验，一定对这些现象并不陌生。在语言模型还不像如今的BERT、GPT这么厉害的时候，这种现象更加明显。
没有经验也没关系，我们来看一个论文里面的例子。输入模型的引文（context)
&ldquo;The study, published in the Proceedings of the They were cattle called Bolivian Cavalleros; they live in a National Academy of Sciences of the United States of remote desert uninterrupted by town, and they speak huge, America (PNAS), was conducted by researchers from the beautiful, paradisiacal Bolivian linguistic thing. They say, Universidad Nacional Autónoma de México (UNAM) and" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.yuanhao.site/post/2020-03-23-beamsearch2/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2020-03-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-03-23T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="十分钟读懂beam search-2"/>
<meta name="twitter:description" content="在上一篇文章中我们介绍了基础版的beam search，这篇文章是对它的一个扩展，可以在模型不改的情况下获得更好的生成结果。今天的介绍围绕的也是一篇蛮新的论文，《The Curious Case of Neural Text Degeneration》，根据这篇论文的版面内容，它应该已经被ICLR 2020接收了。
Beam Search的问题 先解释以下什么要对Beam Search进行改进。因为Beam Search虽然比贪心有所改进，但还是会生成出空洞、重复、前后矛盾的文本。如果你有文本生成经验，一定对这些现象并不陌生。在语言模型还不像如今的BERT、GPT这么厉害的时候，这种现象更加明显。
没有经验也没关系，我们来看一个论文里面的例子。输入模型的引文（context)
&ldquo;The study, published in the Proceedings of the They were cattle called Bolivian Cavalleros; they live in a National Academy of Sciences of the United States of remote desert uninterrupted by town, and they speak huge, America (PNAS), was conducted by researchers from the beautiful, paradisiacal Bolivian linguistic thing. They say, Universidad Nacional Autónoma de México (UNAM) and"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://www.yuanhao.site/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "十分钟读懂beam search-2",
      "item": "https://www.yuanhao.site/post/2020-03-23-beamsearch2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "十分钟读懂beam search-2",
  "name": "十分钟读懂beam search-2",
  "description": "在上一篇文章中我们介绍了基础版的beam search，这篇文章是对它的一个扩展，可以在模型不改的情况下获得更好的生成结果。今天的介绍围绕的也是一篇蛮新的论文，《The Curious Case of Neural Text Degeneration》，根据这篇论文的版面内容，它应该已经被ICLR 2020接收了。\nBeam Search的问题 先解释以下什么要对Beam Search进行改进。因为Beam Search虽然比贪心有所改进，但还是会生成出空洞、重复、前后矛盾的文本。如果你有文本生成经验，一定对这些现象并不陌生。在语言模型还不像如今的BERT、GPT这么厉害的时候，这种现象更加明显。\n没有经验也没关系，我们来看一个论文里面的例子。输入模型的引文（context)\n\u0026ldquo;The study, published in the Proceedings of the They were cattle called Bolivian Cavalleros; they live in a National Academy of Sciences of the United States of remote desert uninterrupted by town, and they speak huge, America (PNAS), was conducted by researchers from the beautiful, paradisiacal Bolivian linguistic thing. They say, Universidad Nacional Autónoma de México (UNAM) and",
  "keywords": [
    "NLP", "文本生成", "beam search"
  ],
  "articleBody": "在上一篇文章中我们介绍了基础版的beam search，这篇文章是对它的一个扩展，可以在模型不改的情况下获得更好的生成结果。今天的介绍围绕的也是一篇蛮新的论文，《The Curious Case of Neural Text Degeneration》，根据这篇论文的版面内容，它应该已经被ICLR 2020接收了。\nBeam Search的问题 先解释以下什么要对Beam Search进行改进。因为Beam Search虽然比贪心有所改进，但还是会生成出空洞、重复、前后矛盾的文本。如果你有文本生成经验，一定对这些现象并不陌生。在语言模型还不像如今的BERT、GPT这么厉害的时候，这种现象更加明显。\n没有经验也没关系，我们来看一个论文里面的例子。输入模型的引文（context)\n“The study, published in the Proceedings of the They were cattle called Bolivian Cavalleros; they live in a National Academy of Sciences of the United States of remote desert uninterrupted by town, and they speak huge, America (PNAS), was conducted by researchers from the beautiful, paradisiacal Bolivian linguistic thing. They say, Universidad Nacional Autónoma de México (UNAM) and\nGPT-2模型， Beam Search, num_beams=32的生成结果\n‘Lunch, marge.’ They don’t tell what the lunch is,” director the Universidad Nacional Autónoma de México Professor Chuperas Omwell told Sky News. “They’ve only (UNAM/Universidad Nacional Autónoma de been talking to scientists, like we’re being interviewed by TV México/Universidad Nacional Autónoma de reporters. We don’t even stick around to be interviewed by México/Universidad Nacional Autónoma de TV reporters. Maybe that’s how they figured out that they’re México/Universidad Nacional Autónoma de …”\n可以发现即使是如今最顶级的语言模型加上足够长的引文输入，还是无法得到高质量的生成结果。\n论文认为这种问题是由于这种试图最大化序列条件概率的解码策略从根上就有问题。他对比了给定同样引文的情况下人类续写和机器生成的词用语言模型计算出来的概率。如下图所示，人类选择的词（橙线）的概率并不是像机器选择的（蓝线）那样总是那些条件概率最大的词。从生成的结果也可以看出，机器生成的结果有大量重复。\n解决对策 人们其实尝试了各种办法对Beam Search进行改进，其实都很好理解，这篇论文总结的也比较到位。\n随机采样 第一种方法是用**随机采样(sampling)**代替取概率最大的词。采样的依据就是解码器输出的词典中每个词的概率分布。相比于按概率“掐尖”，这样会增大所选词的范围，引入更多的随机性。这个方法其实正是我们之前解读过的谷歌开放式聊天机器人Meena采用的方式。当时那篇论文的结论就是这种随机采样的方法远好于Beam Search。但这其实也是有条件的，随机采样容易产生前后不一致的问题。而在开放闲聊领域，生成文本的 长度都比较短 ，这种问题就被自然的淡化了。\n采样的时候有一个可以控制的超参数，称为温度(temperature, $T$)。解码器的输出层后面通常会跟一个softmax函数来将输出概率归一化，通过改变$T$可以控制概率的形貌。softmax的公式如下，当$T$大的时候，概率分布趋向平均，随机性增大；当$T$小的时候，概率密度趋向于集中，即强者俞强，随机性降低，会更多地采样出“放之四海而皆准”的词汇。\n$$p_i=\\frac{\\exp(y_i/T)}{\\sum \\exp(y_i/T)}$$\ntop-k采样 这个方法就是在采样前将输出的概率分布截断，取出概率最大的k个词构成一个集合，然后将这个子集词的概率再归一化，最后重新的概率分布中采样词汇。这个办法据说可以获得比Beam Search好很多的效果，但也有一个问题，就是这个k不太好选。\nWhile top-k sampling leads to considerably higher quality text than either beam search or sampling from the full distribution, the use of a constant k is sub-optimal across varying contexts.\n为啥呢？因为这个概率分布变化比较大，有时候可能很均匀(flat)，有的时候比较集中(peaked)。对于集中的情况还好说，当分布均匀时，一个较小的k容易丢掉很多优质候选词。但如果k定的太大，这个方法又会退化回普通采样。\n核采样（Nucleus sampling) 首先表示我不确定这个翻译是不是对的。\n这是这篇论文提出的方式，也是相比前面那些都更好的采样方式，他不再取一个固定的k，而是固定候选集合的概率密度和在整个概率分布中的比例。也就是构造一个最小候选集$V$，使得\n$$ \\sum_{x \\in V}P(x)\u003ep$$\n选出来这个集合之后也和top-k采样一样，重新归一化集合内词的概率，并把集合外词的概率设为0。这种方式也称为top-p采样。\n论文有一个图，对比了这几种采样方式的效果。\n惩罚重复 为了解决重复问题，还有可以通过惩罚因子将出现过词的概率变小或者强制不使用重复词来解决。惩罚因子来自于同样广为流传的《CTRL: A Conditional Transformer Language Model for Controllable Generation》。如果大家感兴趣的话后面可以专门写一期可控文本生成方向的解读。\n代码解析 其实上述各种采样方式在HuggingFace的库里都已经实现了（感动！），我们来看一下代码。\n先看top-k和top-p采样\n# 代码输入的是logits，而且考虑很周全（我感觉漏了考虑k和p都给了的情况，这应该是不合适的） # 巧妙地使用了torch.cumsum # 避免了一个词都选不出来的尴尬情况 def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\"), min_tokens_to_keep=1): \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering Args: logits: logits distribution shape (batch size, vocabulary size) if top_k \u003e 0: keep only top k tokens with highest probability (top-k filtering). if top_p \u003c 1.0: keep the top tokens with cumulative probability \u003e= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751) Make sure we keep at least min_tokens_to_keep per batch example in the output From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317 \"\"\" if top_k \u003e 0: top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1)) # Safety check # Remove all tokens with a probability less than the last token of the top-k indices_to_remove = logits \u003c torch.topk(logits, top_k)[0][..., -1, None] logits[indices_to_remove] = filter_value if top_p \u003c 1.0: sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold (token with 0 are kept) sorted_indices_to_remove = cumulative_probs \u003e top_p if min_tokens_to_keep \u003e 1: # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below) sorted_indices_to_remove[..., :min_tokens_to_keep] = 0 # Shift the indices to the right to keep also the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 # scatter sorted tensors to original indexing indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove) logits[indices_to_remove] = filter_value return logits 再看看重复惩罚\n# 输入的同样是logits(lprobs) # 同时输入了之前出现过的词以及惩罚系数（大于1的） # 考虑到了logit是正和负时处理方式应该不一样 def enforce_repetition_penalty_(self, lprobs, batch_size, num_beams, prev_output_tokens, repetition_penalty): \"\"\"repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858). \"\"\" for i in range(batch_size * num_beams): for previous_token in set(prev_output_tokens[i].tolist()): # if score \u003c 0 then repetition penalty has to multiplied to reduce the previous token probability if lprobs[i, previous_token] \u003c 0: lprobs[i, previous_token] *= repetition_penalty else: lprobs[i, previous_token] /= repetition_penalty 最后是重复词去除\n# 这个函数将会返回一个不可使用的词表 # 生成n-gram的巧妙方式大家可以借鉴一下 # 下面是一个3-gram的例子 # a = [1,2,3,4,5] # for ngram in zip(*[a[i:] for i in range(3)]): # print(ngram) def calc_banned_tokens(prev_input_ids, num_hypos, no_repeat_ngram_size, cur_len): # Copied from fairseq for no_repeat_ngram in beam_search\"\"\" if cur_len + 1 \u003c no_repeat_ngram_size: # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet return [[] for _ in range(num_hypos)] generated_ngrams = [{} for _ in range(num_hypos)] for idx in range(num_hypos): gen_tokens = prev_input_ids[idx].numpy().tolist() generated_ngram = generated_ngrams[idx] # 就是这巧妙的一句 for ngram in zip(*[gen_tokens[i:] for i in range(no_repeat_ngram_size)]): prev_ngram_tuple = tuple(ngram[:-1]) generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]] def _get_generated_ngrams(hypo_idx): # Before decoding the next token, prevent decoding of ngrams that have already appeared start_idx = cur_len + 1 - no_repeat_ngram_size ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].numpy().tolist()) return generated_ngrams[hypo_idx].get(ngram_idx, []) banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)] return banned_tokens 以上这些代码应该在哪里调用相信看过昨天文章的朋友都应该知道了，这里就放出来最核心的差异。\nif do_sample: # 这是今天的采样方式 _scores = scores + beam_scores[:, None].expand_as(scores) # (batch_size * num_beams, vocab_size) # Top-p/top-k filtering，这一步重建了候选集 _scores = top_k_top_p_filtering( _scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2 ) # (batch_size * num_beams, vocab_size) # re-organize to group the beam together to sample from all beam_idxs _scores = _scores.contiguous().view( batch_size, num_beams * vocab_size ) # (batch_size, num_beams * vocab_size) # Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search) probs = F.softmax(_scores, dim=-1) # 采样 next_tokens = torch.multinomial(probs, num_samples=2 * num_beams) # (batch_size, num_beams * 2) # Compute next scores next_scores = torch.gather(_scores, -1, next_tokens) # (batch_size, num_beams * 2) # sort the sampled vector to make sure that the first num_beams samples are the best next_scores, next_scores_indices = torch.sort(next_scores, descending=True, dim=1) next_tokens = torch.gather(next_tokens, -1, next_scores_indices) # (batch_size, num_beams * 2) else: # 这是昨天的beam search方式 # 直接将log概率相加求条件概率 next_scores = scores + beam_scores[:, None].expand_as(scores) # (batch_size * num_beams, vocab_size) # re-organize to group the beam together (we are keeping top hypothesis accross beams) next_scores = next_scores.view( batch_size, num_beams * vocab_size ) # (batch_size, num_beams * vocab_size) next_scores, next_tokens = torch.topk(next_scores, 2 * num_beams, dim=1, largest=True, sorted=True) OK，今天的文章就到这咯，祝大家生成出高质量的文本！\n",
  "wordCount" : "834",
  "inLanguage": "en",
  "datePublished": "2020-03-23T00:00:00Z",
  "dateModified": "2020-03-23T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.yuanhao.site/post/2020-03-23-beamsearch2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "多头注意力",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.yuanhao.site/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.yuanhao.site" accesskey="h" title="多头注意力 (Alt + H)">多头注意力</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.yuanhao.site/archive" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://www.yuanhao.site">Home</a>&nbsp;»&nbsp;<a href="https://www.yuanhao.site/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      十分钟读懂beam search-2
    </h1>
    <div class="post-meta"><span title='2020-03-23 00:00:00 +0000 UTC'>March 23, 2020</span>&nbsp;·&nbsp;4 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#beam-search%e7%9a%84%e9%97%ae%e9%a2%98" aria-label="Beam Search的问题">Beam Search的问题</a></li>
                <li>
                    <a href="#%e8%a7%a3%e5%86%b3%e5%af%b9%e7%ad%96" aria-label="解决对策">解决对策</a><ul>
                        
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e9%87%87%e6%a0%b7" aria-label="随机采样">随机采样</a></li>
                <li>
                    <a href="#top-k%e9%87%87%e6%a0%b7" aria-label="top-k采样">top-k采样</a></li>
                <li>
                    <a href="#%e6%a0%b8%e9%87%87%e6%a0%b7nucleus-sampling" aria-label="核采样（Nucleus sampling)">核采样（Nucleus sampling)</a></li>
                <li>
                    <a href="#%e6%83%a9%e7%bd%9a%e9%87%8d%e5%a4%8d" aria-label="惩罚重复">惩罚重复</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%bb%a3%e7%a0%81%e8%a7%a3%e6%9e%90" aria-label="代码解析">代码解析</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>在上一篇文章中我们介绍了基础版的beam search，这篇文章是对它的一个扩展，可以在模型不改的情况下获得更好的生成结果。今天的介绍围绕的也是一篇蛮新的论文，<a href="https://arxiv.org/abs/1904.09751" title="The Curious Case of Neural Text Degeneration">《The Curious Case of Neural Text Degeneration》</a>，根据这篇论文的版面内容，它应该已经被ICLR 2020接收了。</p>
<p><img loading="lazy" src="/assets/beamsearch/paper.png" alt="论文截图"  />
</p>
<h2 id="beam-search的问题">Beam Search的问题<a hidden class="anchor" aria-hidden="true" href="#beam-search的问题">#</a></h2>
<p>先解释以下什么要对Beam Search进行改进。因为Beam Search虽然比贪心有所改进，但还是会生成出空洞、重复、前后矛盾的文本。如果你有文本生成经验，一定对这些现象并不陌生。在语言模型还不像如今的BERT、GPT这么厉害的时候，这种现象更加明显。</p>
<p>没有经验也没关系，我们来看一个论文里面的例子。输入模型的引文（context)</p>
<blockquote>
<p>&ldquo;The study, published in the Proceedings of the They were cattle called Bolivian Cavalleros; they live in a National Academy of Sciences of the United States of remote desert uninterrupted by town, and they speak huge, America (PNAS), was conducted by researchers from the
beautiful, paradisiacal Bolivian linguistic thing. They say,
Universidad Nacional Autónoma de México (UNAM) and</p>
</blockquote>
<p>GPT-2模型， Beam Search, num_beams=32的生成结果</p>
<blockquote>
<p>&lsquo;Lunch, marge.&rsquo; They don&rsquo;t tell what the lunch is,&rdquo; director
the Universidad Nacional Autónoma de México
Professor Chuperas Omwell told Sky News. &ldquo;They&rsquo;ve only
(UNAM/Universidad Nacional Autónoma de
been talking to scientists, like we&rsquo;re being interviewed by TV
México/Universidad Nacional Autónoma de
reporters. We don&rsquo;t even stick around to be interviewed by
México/Universidad Nacional Autónoma de
TV reporters. Maybe that&rsquo;s how they figured out that they&rsquo;re
México/Universidad Nacional Autónoma de &hellip;”</p>
</blockquote>
<p>可以发现即使是如今最顶级的语言模型加上足够长的引文输入，还是无法得到高质量的生成结果。</p>
<p>论文认为这种问题是由于这种试图最大化序列条件概率的解码策略从根上就有问题。他对比了给定同样引文的情况下人类续写和机器生成的词用语言模型计算出来的概率。如下图所示，人类选择的词（橙线）的概率并不是像机器选择的（蓝线）那样总是那些条件概率最大的词。从生成的结果也可以看出，机器生成的结果有大量重复。</p>
<p><img loading="lazy" src="/assets/beamsearch/probability.png" alt="概率对比图"  />
</p>
<h2 id="解决对策">解决对策<a hidden class="anchor" aria-hidden="true" href="#解决对策">#</a></h2>
<p>人们其实尝试了各种办法对Beam Search进行改进，其实都很好理解，这篇论文总结的也比较到位。</p>
<h3 id="随机采样">随机采样<a hidden class="anchor" aria-hidden="true" href="#随机采样">#</a></h3>
<p>第一种方法是用**随机采样(sampling)**代替取概率最大的词。采样的依据就是解码器输出的词典中每个词的概率分布。相比于按概率“掐尖”，这样会增大所选词的范围，引入更多的随机性。这个方法其实正是我们之前解读过的谷歌开放式聊天机器人Meena采用的方式。当时那篇论文的结论就是这种随机采样的方法远好于Beam Search。但这其实也是有条件的，随机采样容易产生前后不一致的问题。而在开放闲聊领域，生成文本的 <strong>长度都比较短</strong> ，这种问题就被自然的淡化了。</p>
<p>采样的时候有一个可以控制的超参数，称为<strong>温度</strong>(temperature, $T$)。解码器的输出层后面通常会跟一个softmax函数来将输出概率归一化，通过改变$T$可以控制概率的形貌。softmax的公式如下，当$T$大的时候，概率分布趋向平均，随机性增大；当$T$小的时候，概率密度趋向于集中，即强者俞强，随机性降低，会更多地采样出“放之四海而皆准”的词汇。</p>
<p>$$p_i=\frac{\exp(y_i/T)}{\sum \exp(y_i/T)}$$</p>
<h3 id="top-k采样">top-k采样<a hidden class="anchor" aria-hidden="true" href="#top-k采样">#</a></h3>
<p>这个方法就是在采样前将输出的概率分布截断，取出概率最大的k个词构成一个集合，然后将这个子集词的概率再归一化，最后重新的概率分布中采样词汇。这个办法据说可以获得比Beam Search好很多的效果，但也有一个问题，就是这个k不太好选。</p>
<blockquote>
<p>While top-k sampling leads to considerably higher quality text than either beam search or sampling from the full distribution, the use of a constant k is sub-optimal across varying contexts.</p>
</blockquote>
<p>为啥呢？因为这个概率分布变化比较大，有时候可能很均匀(flat)，有的时候比较集中(peaked)。对于集中的情况还好说，当分布均匀时，一个较小的k容易丢掉很多优质候选词。但如果k定的太大，这个方法又会退化回普通采样。</p>
<p><img loading="lazy" src="/assets/beamsearch/distribution.png" alt="两种分布，左边是均匀的，右边是集中的"  />
</p>
<h3 id="核采样nucleus-sampling">核采样（Nucleus sampling)<a hidden class="anchor" aria-hidden="true" href="#核采样nucleus-sampling">#</a></h3>
<p>首先表示我不确定这个翻译是不是对的。</p>
<p>这是这篇论文提出的方式，也是相比前面那些都更好的采样方式，他不再取一个固定的k，而是固定候选集合的概率密度和在整个概率分布中的比例。也就是构造一个<strong>最小</strong>候选集$V$，使得</p>
<p>$$ \sum_{x \in V}P(x)&gt;p$$</p>
<p>选出来这个集合之后也和top-k采样一样，重新归一化集合内词的概率，并把集合外词的概率设为0。这种方式也称为top-p采样。</p>
<p>论文有一个图，对比了这几种采样方式的效果。</p>
<p><img loading="lazy" src="/assets/beamsearch/sample.png" alt="效果对比图，红字是前后不符，蓝字是重复。Nucleus效果拔群。"  />
</p>
<h3 id="惩罚重复">惩罚重复<a hidden class="anchor" aria-hidden="true" href="#惩罚重复">#</a></h3>
<p>为了解决重复问题，还有可以通过<strong>惩罚因子</strong>将出现过词的概率变小或者<strong>强制不使用重复词</strong>来解决。惩罚因子来自于同样广为流传的<a href="https://arxiv.org/abs/1909.05858" title="CTRL: A Conditional Transformer Language Model for Controllable Generation">《CTRL: A Conditional Transformer Language Model for Controllable Generation》</a>。如果大家感兴趣的话后面可以专门写一期可控文本生成方向的解读。</p>
<h2 id="代码解析">代码解析<a hidden class="anchor" aria-hidden="true" href="#代码解析">#</a></h2>
<p>其实上述各种采样方式在HuggingFace的库里都已经实现了（感动！），我们来看一下代码。</p>
<p>先看top-k和top-p采样</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 代码输入的是logits，而且考虑很周全（我感觉漏了考虑k和p都给了的情况，这应该是不合适的）</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 巧妙地使用了torch.cumsum</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 避免了一个词都选不出来的尴尬情况</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">top_k_top_p_filtering</span>(logits, top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, filter_value<span style="color:#f92672">=-</span>float(<span style="color:#e6db74">&#34;Inf&#34;</span>), min_tokens_to_keep<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34; Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            logits: logits distribution shape (batch size, vocabulary size)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            if top_k &gt; 0: keep only top k tokens with highest probability (top-k filtering).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            if top_p &lt; 1.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Make sure we keep at least min_tokens_to_keep per batch example in the output
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> top_k <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        top_k <span style="color:#f92672">=</span> min(max(top_k, min_tokens_to_keep), logits<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))  <span style="color:#75715e"># Safety check</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Remove all tokens with a probability less than the last token of the top-k</span>
</span></span><span style="display:flex;"><span>        indices_to_remove <span style="color:#f92672">=</span> logits <span style="color:#f92672">&lt;</span> torch<span style="color:#f92672">.</span>topk(logits, top_k)[<span style="color:#ae81ff">0</span>][<span style="color:#f92672">...</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>        logits[indices_to_remove] <span style="color:#f92672">=</span> filter_value
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> top_p <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">1.0</span>:
</span></span><span style="display:flex;"><span>        sorted_logits, sorted_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sort(logits, descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        cumulative_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumsum(F<span style="color:#f92672">.</span>softmax(sorted_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Remove tokens with cumulative probability above the threshold (token with 0 are kept)</span>
</span></span><span style="display:flex;"><span>        sorted_indices_to_remove <span style="color:#f92672">=</span> cumulative_probs <span style="color:#f92672">&gt;</span> top_p
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> min_tokens_to_keep <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)</span>
</span></span><span style="display:flex;"><span>            sorted_indices_to_remove[<span style="color:#f92672">...</span>, :min_tokens_to_keep] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Shift the indices to the right to keep also the first token above the threshold</span>
</span></span><span style="display:flex;"><span>        sorted_indices_to_remove[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">1</span>:] <span style="color:#f92672">=</span> sorted_indices_to_remove[<span style="color:#f92672">...</span>, :<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>        sorted_indices_to_remove[<span style="color:#f92672">...</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># scatter sorted tensors to original indexing</span>
</span></span><span style="display:flex;"><span>        indices_to_remove <span style="color:#f92672">=</span> sorted_indices_to_remove<span style="color:#f92672">.</span>scatter(<span style="color:#ae81ff">1</span>, sorted_indices, sorted_indices_to_remove)
</span></span><span style="display:flex;"><span>        logits[indices_to_remove] <span style="color:#f92672">=</span> filter_value
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> logits
</span></span></code></pre></div><p>再看看重复惩罚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 输入的同样是logits(lprobs)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 同时输入了之前出现过的词以及惩罚系数（大于1的）</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 考虑到了logit是正和负时处理方式应该不一样</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">enforce_repetition_penalty_</span>(self, lprobs, batch_size, num_beams, prev_output_tokens, repetition_penalty):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858). &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(batch_size <span style="color:#f92672">*</span> num_beams):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> previous_token <span style="color:#f92672">in</span> set(prev_output_tokens[i]<span style="color:#f92672">.</span>tolist()):
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># if score &lt; 0 then repetition penalty has to multiplied to reduce the previous token probability</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> lprobs[i, previous_token] <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                    lprobs[i, previous_token] <span style="color:#f92672">*=</span> repetition_penalty
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                    lprobs[i, previous_token] <span style="color:#f92672">/=</span> repetition_penalty
</span></span></code></pre></div><p>最后是重复词去除</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 这个函数将会返回一个不可使用的词表</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成n-gram的巧妙方式大家可以借鉴一下</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 下面是一个3-gram的例子</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># a = [1,2,3,4,5]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># for ngram in zip(*[a[i:] for i in range(3)]):</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#    print(ngram)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calc_banned_tokens</span>(prev_input_ids, num_hypos, no_repeat_ngram_size, cur_len):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Copied from fairseq for no_repeat_ngram in beam_search&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> cur_len <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;</span> no_repeat_ngram_size:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># return no banned tokens if we haven&#39;t generated no_repeat_ngram_size tokens yet</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> [[] <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_hypos)]
</span></span><span style="display:flex;"><span>    generated_ngrams <span style="color:#f92672">=</span> [{} <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_hypos)]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> idx <span style="color:#f92672">in</span> range(num_hypos):
</span></span><span style="display:flex;"><span>        gen_tokens <span style="color:#f92672">=</span> prev_input_ids[idx]<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>        generated_ngram <span style="color:#f92672">=</span> generated_ngrams[idx]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 就是这巧妙的一句</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> ngram <span style="color:#f92672">in</span> zip(<span style="color:#f92672">*</span>[gen_tokens[i:] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(no_repeat_ngram_size)]):
</span></span><span style="display:flex;"><span>            prev_ngram_tuple <span style="color:#f92672">=</span> tuple(ngram[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>            generated_ngram[prev_ngram_tuple] <span style="color:#f92672">=</span> generated_ngram<span style="color:#f92672">.</span>get(prev_ngram_tuple, []) <span style="color:#f92672">+</span> [ngram[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_get_generated_ngrams</span>(hypo_idx):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Before decoding the next token, prevent decoding of ngrams that have already appeared</span>
</span></span><span style="display:flex;"><span>        start_idx <span style="color:#f92672">=</span> cur_len <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> no_repeat_ngram_size
</span></span><span style="display:flex;"><span>        ngram_idx <span style="color:#f92672">=</span> tuple(prev_input_ids[hypo_idx, start_idx:cur_len]<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>tolist())
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> generated_ngrams[hypo_idx]<span style="color:#f92672">.</span>get(ngram_idx, [])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    banned_tokens <span style="color:#f92672">=</span> [_get_generated_ngrams(hypo_idx) <span style="color:#66d9ef">for</span> hypo_idx <span style="color:#f92672">in</span> range(num_hypos)]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> banned_tokens
</span></span></code></pre></div><p>以上这些代码应该在哪里调用相信看过昨天文章的朋友都应该知道了，这里就放出来最核心的差异。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> do_sample:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 这是今天的采样方式</span>
</span></span><span style="display:flex;"><span>    _scores <span style="color:#f92672">=</span> scores <span style="color:#f92672">+</span> beam_scores[:, <span style="color:#66d9ef">None</span>]<span style="color:#f92672">.</span>expand_as(scores)  <span style="color:#75715e"># (batch_size * num_beams, vocab_size)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Top-p/top-k filtering，这一步重建了候选集</span>
</span></span><span style="display:flex;"><span>    _scores <span style="color:#f92672">=</span> top_k_top_p_filtering(
</span></span><span style="display:flex;"><span>        _scores, top_k<span style="color:#f92672">=</span>top_k, top_p<span style="color:#f92672">=</span>top_p, min_tokens_to_keep<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>    )  <span style="color:#75715e"># (batch_size * num_beams, vocab_size)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># re-organize to group the beam together to sample from all beam_idxs</span>
</span></span><span style="display:flex;"><span>    _scores <span style="color:#f92672">=</span> _scores<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(
</span></span><span style="display:flex;"><span>        batch_size, num_beams <span style="color:#f92672">*</span> vocab_size
</span></span><span style="display:flex;"><span>    )  <span style="color:#75715e"># (batch_size, num_beams * vocab_size)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)</span>
</span></span><span style="display:flex;"><span>    probs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(_scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 采样</span>
</span></span><span style="display:flex;"><span>    next_tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probs, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> num_beams)  <span style="color:#75715e"># (batch_size, num_beams * 2)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute next scores</span>
</span></span><span style="display:flex;"><span>    next_scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>gather(_scores, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, next_tokens)  <span style="color:#75715e"># (batch_size, num_beams * 2)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># sort the sampled vector to make sure that the first num_beams samples are the best</span>
</span></span><span style="display:flex;"><span>    next_scores, next_scores_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sort(next_scores, descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    next_tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>gather(next_tokens, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, next_scores_indices)  <span style="color:#75715e"># (batch_size, num_beams * 2)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 这是昨天的beam search方式</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 直接将log概率相加求条件概率</span>
</span></span><span style="display:flex;"><span>    next_scores <span style="color:#f92672">=</span> scores <span style="color:#f92672">+</span> beam_scores[:, <span style="color:#66d9ef">None</span>]<span style="color:#f92672">.</span>expand_as(scores)  <span style="color:#75715e"># (batch_size * num_beams, vocab_size)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># re-organize to group the beam together (we are keeping top hypothesis accross beams)</span>
</span></span><span style="display:flex;"><span>    next_scores <span style="color:#f92672">=</span> next_scores<span style="color:#f92672">.</span>view(
</span></span><span style="display:flex;"><span>        batch_size, num_beams <span style="color:#f92672">*</span> vocab_size
</span></span><span style="display:flex;"><span>    )  <span style="color:#75715e"># (batch_size, num_beams * vocab_size)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    next_scores, next_tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(next_scores, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> num_beams, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, largest<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, sorted<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>OK，今天的文章就到这咯，祝大家生成出高质量的文本！</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.yuanhao.site/tags/nlp/">NLP</a></li>
      <li><a href="https://www.yuanhao.site/tags/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">文本生成</a></li>
      <li><a href="https://www.yuanhao.site/tags/beam-search/">beam search</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://www.yuanhao.site/post/2020-05-11-xlm/">
    <span class="title">« Prev</span>
    <br>
    <span>跨语种语言模型</span>
  </a>
  <a class="next" href="https://www.yuanhao.site/post/2020-03-20-beamsearch1/">
    <span class="title">Next »</span>
    <br>
    <span>十分钟读懂beam search-1</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 十分钟读懂beam search-2 on x"
            href="https://x.com/intent/tweet/?text=%e5%8d%81%e5%88%86%e9%92%9f%e8%af%bb%e6%87%82beam%20search-2&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f&amp;hashtags=NLP%2c%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90%2cbeamsearch">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 十分钟读懂beam search-2 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f&amp;title=%e5%8d%81%e5%88%86%e9%92%9f%e8%af%bb%e6%87%82beam%20search-2&amp;summary=%e5%8d%81%e5%88%86%e9%92%9f%e8%af%bb%e6%87%82beam%20search-2&amp;source=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 十分钟读懂beam search-2 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f&title=%e5%8d%81%e5%88%86%e9%92%9f%e8%af%bb%e6%87%82beam%20search-2">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 十分钟读懂beam search-2 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 十分钟读懂beam search-2 on whatsapp"
            href="https://api.whatsapp.com/send?text=%e5%8d%81%e5%88%86%e9%92%9f%e8%af%bb%e6%87%82beam%20search-2%20-%20https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 十分钟读懂beam search-2 on telegram"
            href="https://telegram.me/share/url?text=%e5%8d%81%e5%88%86%e9%92%9f%e8%af%bb%e6%87%82beam%20search-2&amp;url=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 十分钟读懂beam search-2 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=%e5%8d%81%e5%88%86%e9%92%9f%e8%af%bb%e6%87%82beam%20search-2&u=https%3a%2f%2fwww.yuanhao.site%2fpost%2f2020-03-23-beamsearch2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://www.yuanhao.site">多头注意力</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
