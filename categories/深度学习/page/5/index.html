<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>深度学习 | 多头注意力</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://www.yuanhao.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.yuanhao.site/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.yuanhao.site/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.yuanhao.site/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.yuanhao.site/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.yuanhao.site/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://www.yuanhao.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="深度学习" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://www.yuanhao.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="深度学习"/>
<meta name="twitter:description" content=""/>

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.yuanhao.site" accesskey="h" title="多头注意力 (Alt + H)">多头注意力</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.yuanhao.site/archive" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://www.yuanhao.site/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header"><div class="breadcrumbs"><a href="https://www.yuanhao.site">Home</a>&nbsp;»&nbsp;<a href="https://www.yuanhao.site/categories/">Categories</a></div>
  <h1>
    深度学习
  </h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">最近邻搜索，MIPS，LSH和ALSH
    </h2>
  </header>
  <div class="entry-content">
    <p>上一篇介绍REALM的文章对文中提到的Maximum Inner Product Search没有作充分的介绍。发出去的标题已经没法改了，这篇文章介绍一下MIPS和最近邻搜索问题，以及两个相关的算法。
问题定义 MIPS的定义很简单，假设你有一堆d维向量，组成集合X，现在输入了一个同样维度的查询向量q(query)，请从X中找出一个p，使得p和q的点积在集合X是最大的。用公式写出来就是
$$p=\mathop{\arg\max}_{x \in X}x^Tq$$
这个问题和最近邻问题很像，最近邻问题只要把上面的定义改成找一个p使得p和q的距离最小，假设这个距离是欧氏距离，则
$$p = \mathop{\arg\min}_{x \in X}\left |q-x\right|^2=(\left | x \right |^2 – 2q^Tx)$$
如果X中的向量模长都一样，那两个问题其实是等价的。然而在很多实际场景例如BERT编码后的句向量、推荐系统里的各种Embedding等，这个约束是不满足的。
最近邻搜索其实应用非常广泛，如图片检索、推荐系统、问答等等。以问答匹配为例，虽然我们可以用BERT这样的大型模型获得很好的准确度，但如果用BERT直接对语料库中的所有问题进行计算，将耗费大量的时间。所以可以先用关键词检索或者向量检索从语料库里召回一些候选语料后再做高精度匹配。
朴素的算法 对于MIPS问题，一个直观的蛮力算法就是计算出所有相关的内积，然后将内积排序，找到最大的那个。对于最近邻问题其实也类似，即使X中向量模长各不相同，也可以提前计算出来，并不会增加排序的时间复杂度。
内积的计算可以转换成一个矩阵乘法，在CPU和GPU上都有大量的高效实现。当X中有N个向量时，时间复杂度是O(Nd)，当N不大的时候是可以接受的，但是通常在工业界的大规模系统中，X的规模往往很大，朴素算法就显得力不从心。
Locality-sensitive hashing 对于某些距离度量（例如欧式距离，cosine距离）下的最近邻问题，可以使用LSH算法来解决。LSH的思路就像下图示意的那样，用hash函数把高维空间的点分到几个桶里去，从而减少距离的计算量。
跟普通的哈希函数不同，这个哈希函数是Locality-sensitive的。具体地说就是它有一个神奇的特点：在空间中离得近的点被分到同一个桶的概率大，离得远的点则大概率被分到不同的桶里去。或者说对于两个点x和y，他们被哈希函数分到同一个桶的概率随着距离的增大单调递减。
这样在查询的时候，只需要精确地比较和查询向量q处在同一个桶里的那些x。如果桶足够多，那便可以将N大大降低，从而提高查询速度。但需要注意的是，LSH是一个近似算法，有可能产生桶内的向量其实都不是最优解的情况，不同哈希函数发生这种情况的概率都不一样，也是作为评价哈希函数好坏的重要依据之一，对这部分感兴趣的朋友可以读参考文献。
下面举一个具体的例子来解释一下LSH。假设某个最近邻问题考虑的距离度量是cosine距离，有一个满足要求的LSH函数（变换），称为Random Projection。
如上图所示，其过程很好理解：
随机取一个空间中的超平面将空间分为两半，X内位于某一半的点标为0，其他标为1； 重复第一步K次。 完成之后，X中的每个点便得到了一个由K个0，1组成的表示（signature）。例如重复了K=32次，那每个点都被分到了一个用一个int32类型的整数编号的桶里。如果这些点在空间中分布足够均匀，那么我们将可以期望每个桶里只有N/2^K个点，当K～logN，则查询的时间复杂度就约为O(dlogN)。整个过程构建出了一张哈希表，由于LSH可能会错过最优解，一个可行的增强鲁棒性的做法是用同样的方法多构造几张哈希表，借助随机的力量来降低犯错的概率。这里有一个讲解LSH的视频，可谓短小精悍，直观易懂，推荐给大家。
LSH看上去相对于朴素算法确实前进了一大步。但别高兴得太早，要达到O(dlogN)的效果必须服从那个很强的假设。而点在空间中分布足够均匀往往是不太现实的。除此之外，一个LSH只能适用于某些距离度量，对于MIPS，找不到符合要求的LSH。
Asymmetric LSH(ALSH) 论文里证明了找不到可用于MIPS问题的LSH函数，但他们发现对LSH稍作点改动即可将MIPS问题转变为欧式距离下的最近邻搜索问题。改动的关键就在于Asymmetric这个词。在LSH算法中，对查询向量q和X中的向量做的是同样（对称） 的变换，而在ALSH中作者对两者使用了 不同（非对称） 的变换。简单起见，假设查询向量q的模长是1。对于X，先做一个放缩变换使得X中所有向量x的所有元素都小于1。然后对X中的向量进行变换P(x)，对查询向量q做变换Q(x)，P和Q的定义如下：
$$P(x) = [x; \left | x \right |_2^{2^1}; \left | x \right |_2^{2^2},…,\left | x \right |_2^{2^m}]\ Q(x) = [x;\frac{1}{2};\frac{1}{2},…,\frac{1}{2}]$$
可以发现，P和Q虽然变换不同，但都会使输入向量增加m维。进一步观察可以得到
$$Q(q)^TP(x)=q^Tx&#43;\frac{1}{2}(\left | x \right |_2^{2^1}&#43; \left | x \right |_2^{2^2}]&#43;…&#43;\left | x \right |_2^{2^m})\ \left | P(x) \right |_2^2=\left | x \right |_2^{2^1}&#43; \left | x \right |_2^{2^2}]&#43;…&#43;\left | x \right |_2^{2^{m&#43;1}}$$...</p>
  </div>
  <footer class="entry-footer"><span title='2020-03-07 00:00:00 +0000 UTC'>March 7, 2020</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 最近邻搜索，MIPS，LSH和ALSH" href="https://www.yuanhao.site/post/2020-03-07-mips/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">REALM: Retrieval-Augmented Language Model Pre Training
    </h2>
  </header>
  <div class="entry-content">
    <p>知识就是力量 ——培根
背景 去年可以说是语言模型快速发展的一年，BERT、XLNET、Albert等等模型不断刷新各个NLP榜单。在NLP榜单中比较引人注目的应该属于阅读理解型的任务，例如SQuAD等等。以SQuAD为例，模型需要阅读一段给定的文本，然后回答几个问题，问题如果存在答案，答案一定可以在文章中找到。所以说虽然叫阅读理解，但其实和序列标注有点相像，是在给定序列中标出答案段。而这篇论文针对的问题叫开放领域问答（Open-domain QA），对于一个问题Q，模型需要从包含大量文档的知识库中找到答案，而不是像SQuAD数据集一样从一篇文章中寻找。
大部分的语言模型都采用一种称为masked language model，简称MLM的任务来训练，让模型学会类似完形填空一样的能力。通过在大规模语料上的训练，预训练语言模型如BERT实际上已经隐含了一些知识。例如输入一句“The is the currency of the United Kingdom”，BERT很有可能会填入单词&#34;pound&#34;。虽然他还是根据词的共现信息学习和推理的，但看上去就像具有所谓的知识一样。从去年开始就有越来越多的研究从单纯语言模型转换为带有知识嵌入的语言模型，例如清华和百度提出的两个同名模型ERNIE。
但上面说的这种隐含知识不好把握，也难以扩展。这篇论文则提出了一种更加模块化且可解释性更强的知识嵌入方法。总的来说，他的方法是训练一个独立的“语境知识抽取器”（contextual knowledge retriever），通过这个抽取器来决定应该在推理时使用哪些知识。而且这个抽取器和语言模型一起进行非监督预训练大大提高模型性能。
方法 如上图所示，整篇论文涉及两个任务，左边是语言模型预训练任务MLM，右边是QA任务。下图是预训练任务一个更加完整的流程图，我们由此切入进行介绍。
整个过程分为两个关键步骤。先看第一步，即neural knowledge retriever，它负责计算p(z|x)。要实现这个过程首先需要对z和x进行编码。论文采用的是BERT，对于问题x，直接输入BERT，取[CLS] token的输出作为编码向量，而对于文档z，则将标题和正文用[SEP]连接后输入BERT，同样去[CLS] token的输出。论文中还对BERT的输出向量进行了降维处理。即
对于文档库中的某一个z，则
其中f是问题和文档的相关性，
以上部分就称为neural knowledge retriever，通过他每篇z都会得到一个p。现在可以进行第二步，综合x和z求y。上图是一个预训练的例子，y是抠掉的词。利用z的方式是将z的正文和x拼在一起来提供上下文信息，然后优化下面的目标
其中j指第j个masked token。
在做QA的时候稍有不同。由于此时是针对某个具体的z，所以作者将开放域问答任务退化成了像SQuAD一样在文档中找答案的阅读理解任务。
这一部分就是knowledge-augmented encoder。
训练 上面已经描述了预训练阶段和QA finetune阶段的任务。训练的过程都是最大化正确y对应的logp(y|z,x)，而且以上描述的两个任务都是可以端到端优化的。
但这里面对一个问题，上面有个公式需要对整个知识库中所有的文档z的相关概率求和，这是很困难的。作者提出将这一步用只对概率最高的k个文档计算来近似，因为绝大部分文档由于与问题不相关，p(z|x)都非常小。但问题还没有解决，如何找到概率最高的k个文档呢。
观察公式可以发现p(z|x)是正比于两个编码后的内积的，由于大家的分母都一样，分子的顺序就是整个分数的顺序。所以可以用最大内积搜索算法（Maximum Inner Product Search, MIPS，并不知道是什么，维基百科都没有）来解决。但要构建一个快速检索的索引又要求两个编码后的向量是确定的，而由于编码器是不断训练的，所以这个条件无法满足。为了追求一个平衡，作者决定每隔几百步才更新一下编码器，并重新构建索引。而且这只发生在预训练语言模型的时候，在finetune QA任务的时候只使用语言模型得到的编码器编码一次所有的z和x并构建索引。
额外策略 在研究过程中作者发现了一些能让模型更好训练的策略。
只训练真正需要知识的词（通常是实体和日期）来训练MLM 在topk文档外添加一个虚拟的null document 避免让x出现在z中（因为x被mask过，如果它来源于z，那答案就暴露了！） 避免冷启动的retriever太渣导致的恶性循环，他们用了一个以ICT作为任务的模型来初始化retriever 结果对比 这篇论文的对手主要是原来sparse retriever&#43;神经阅读理解模型的组合，例如大名鼎鼎的DrQA。所谓sparse retriever就是指用例如TFIDF之类的特征进行检索的模型。还有一些跟本文很像的neural retriever&#43;neural reader的组合。其中提到了一个ORQA，跟这篇非常像，只是这篇增加了预训练的步骤。最后是一些生成式模型，例如finetune后的T5（可怕！）
在Natural Questions-Open（NQ）、Web Questions（WQ）和Curated Trec（CT）三个数据集上的结果如下 总之一句话，非常牛逼！而且这里模型只取了top 5文档，其他模型可能取了20-80篇，还是打不过他。注意到ours的括号内有两个数据，Z是知识库，很好理解，X是指预训练用的语料。而且通过文章的Ablation Analysis部分可以知道预训练其实是非常关键的一个步骤，对performance的贡献非常大。
后记 我感觉这篇论文和他提到的ORQA还是很厉害的，知识嵌入也从去年的实体粒度的嵌入发展到了如今句子、篇章级别的嵌入。试想一下，这项技术发展起来之后，如今基于词的Sparse搜索引擎应该很快会发展成对NN更友好的Dense搜索引擎，所有的内容也许都会被映射到一个向量空间。各路神经网络将在这个向量空间尽情驰骋~莫非得encoder者得天下？！
论文链接：https://kentonl.com/pub/gltpc.2020.pdf</p>
  </div>
  <footer class="entry-footer"><span title='2020-03-06 00:00:00 +0000 UTC'>March 6, 2020</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to REALM: Retrieval-Augmented Language Model Pre Training" href="https://www.yuanhao.site/post/2020-03-06-realm/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">能跟你聊DOTA的神经对话模型：Meena&amp;DialoGPT
    </h2>
  </header>
  <div class="entry-content">
    <p>提到对话机器人或者聊天机器人，国内的朋友可能先想到的是微软小冰或者苹果Siri。这两个机器人由于需要完成一些功能性的任务，都采用了模块化设计，虽然神经网络在其中起到了重要作用，但输出结果并不是从输入语句“端到端”产生的。而且用过的朋友都知道，他们的聊天能力并不是很令人满意。
今天介绍的神经对话模型则是一步到位，将对话历史作为输入，让模型直接生成下一轮的回复。学术一点说，神经对话模型本质上是在解决文本生成的问题。相比于很多聊天机器人（包括我们的晴天一号）目前使用的语料库&#43;检索算法的架构，生成式（包括改写等等）算法能给对话带来更多的可能性和趣味性。最近，得益于大规模预训练语言模型，文本生成任务得到了长足的发展。本文的主角就代表了目前神经对话模型的最高水平，分别是19年十一月由微软发布的DialoGPT和20年一月谷歌发布的Meena。
在开始正文之前，先展示一段机器人生成的对话，让大家对技术现状有个直观感受。
模型 两篇论文都没有在模型方面有什么重大的创新，他们使用的都是如下图所示基于Transformer的seq2seq模型。这里大致介绍一下所谓seq2seq模型的工作原理。每生成一个单词的过程如下：将对话的历史信息输入进编码器（左下角inputs），模型已经生成的当前轮结果输入解码器（右下角outputs，没错，outputs是也用来input的~），然后模型将会综合两者信息输出它预测的下一个词（上方output）。而生成一句完整的回复只需要将刚才新生成的词拼接到当前轮已有结果的后面（图中shifted right的含义），重复上述过程直到模型输出句子结束符。
DialoGPT采用的是标准的GPT-2模型，也就是上图模型的加大号版本；而Meena采用的是去年Google通过神经网络结构搜索（NAS）方法得到的进化版transformer模型（Evolved Transformer）。Meena由1个ET编码器和13个ET解码器构成，ET解码器和标准Transformer解码器的对比如下图所示，两者似乎差别不大，论文注脚说一个ET Decoder Block大概是标准Transformer Decoder Block的两倍深。
虽然Meena和DialoGPT都是Transformer模块搭建的模型，但两者规模的差别非常巨大。Meena使用了极其巨大的隐层规模（2560 hidden size，32 attention heads)，总参数量达到了惊人的2.6B。相比之下微软的模型就克制很多，最大规模的DialoGPT参数量为762M，表现最好的模型参数量是345M。作为参照，目前地表最强——google的T5模型参数量是11B，BERT large则只有340M。
数据 数据则是两篇论文提升performance的重中之重，特别是对于模型大小比较正常的DialoGPT来说。 DialoGPT论文里明确说数据是从Reddit上搞下来的。通过解析Reddit上的讨论帖，可以获得“树状”的对话数据，然后把每一条从根节点到叶节点的路径拎出来都能获得一个完整的对话。假设一个对话共说了K轮，每次取出当前轮作为标签，之前轮的对话作为输入，总共可以获得K-1条训练数据。Meena的数据来自于public domain social media conversations，他也是从树状对话中获得的数据，我猜测这些对话的形式应该和reddit这种论坛比较相似。
他们的数据量都是非常之巨大的，DialoGPT的总session数是147116725（147M），总词数是1.8B。再来看Meena，他们的总训练数据量是867M，总词数是40B，存成文本文件是341GB。这里可以对比一下之前的模型，BERT的训练数据是16GB，GPT-2的训练数据是40GB，曾经的数据狂魔Roberta是160GB。谷歌再次定义了什么叫大力出奇迹。我在维基百科看到人的平均阅读速度是每分钟200词，按这样的速度1 Billion单词需要一个人不眠不休阅读大概347天。从这个角度看，目前的训练数据量似乎又显得比较合理了。
两篇论文都提到要对数据做非常大刀阔斧的清洗，例如删掉太长或太短的、重复词语多的、含有URL的、不含有常见词的等等。总之，要保证数据质量足够高。
顺便提一下，DialoGPT尝试了从预训练模型迁移和从对话数据集从头训练两种方式。结果是迁移的表现明显好于从头训练。我认为这个现象非常的合理，使用预训练模型就好比教一个学过好几年语文的小朋友在网上聊天，这应该好过一个从小就只通过网络聊天学语文的小朋友。但Meena好像采用的是从头训练的模式，也许他们预训练一把能得到更好的结果。
由于使用了极大的模型和数据集，两个模型都是使用了豪华的硬件来进行训练。DialoGPT使用16个V100，而Meena则使用了2048个TPU v3核训练了30天。512个v2 TPU组成的POD三年协议价是12万美元一个月，v3应该更贵，所以你如果想复现这个模型，至少要花费48万美元。有钱真好:)
解码方式 解码是生成式任务里很重要的一个部分。因为正如前面介绍的，生成模型在推理时每一个时间步只会生成下一个词，而且后面的结果会依赖前面的结果，所以就需要一个好的解码策略来保证这一个个token最后能组成高质量的句子。去年有不少关于解码的工作，希望让机器人产生更有意义、有个性的回复。
DialoGPT没有采用什么独特的解码方式，而是使用了常见的Beam Search（集束搜索），只在最后用了一个较为新颖的重排序算法来从集束搜索得到的TOP K个结果中选出最佳的那个。在排序时，他们用一个训练好的“反向模型”来由生成的回复倒推输入。选用使输入语句概率最大的那个输出作为最终的输出。由于采用了Beam Search，它的解码过程应该是比较耗时的。
Meena比较有意思，作者们表示由于模型的Perplexity（困惑度）够低，完全不需要使用集束搜索。而是采用非常简洁的采样&#43;重排就可以得到好的句子。
困惑度p可以理解为，如果每个时间步都根据语言模型计算的概率分布随机挑词，那么平均情况下，挑多少个词才能挑到正确的那个。 –知乎用户TimsonShi
所谓采样，就是在每一步都根据输出层得到的概率分布，随机选一个输出token，直到得出一个完整的句子。重复采用过程N次，得能到N个句子，最后将这N句话根据归一化后的句子概率排序，选出最优的。需要注意的是他们的输出层并不是对Vocabulary的简单Softmax，而是像知识蒸馏里一样增加了采样温度，即
$$p_i=\frac{\exp(z_j/T)}{\sum_j{\exp(z_j/T)}}$$
作者们发现T的大小会影响生成结果。T小的时候，倾向于使用常规词汇，而T大的时候倾向于使用与上下文相关的词汇。论文使用的T为0.88，N为20。他们的结果显示这种采样的效果相比于集束搜索确实有很大的改进。下面一个是集束搜索的结果，另一个是采样，采样很好地避免了集束搜索生成结果丰富性差的弊端。
能得到这么好的结果有一个重要前提，就是模型的困惑度也就是perplexity够低。Meena的困惑度有多低呢，它是10.2，相比较之下DialoGPT的困惑度大约是15。 评价指标与结果
对评价指标的选择，两篇论文有很多共同点。他们都已经从BLEU这种考量词级别重合度的离散客观指标进化到了直接跟对话质量相关的抽象指标。从这一点也可以感觉出这个领域已经进入了相对成熟的阶段。DialoGPT使用人工评价的方式，评价模型的relevance，informativeness和how human-like。而Meena则用综合sensibleness和specificity的称为SSA的指标对机器人进行评价。所谓SSA就是对sensibleness和specificity的简单平均，A对应就是average。
Sensibleness我认为基本对应relevance，是指机器人能否得出符合上下文的有意义回答。但有意义是不够的，正如文章所说，一个只会回答“I don’t know”这类万金油句子的机器人说的话也是符合上下文的，但其实并不好。
Specificity对应infomativeness，是另一个维度，它评估机器人是否能给出有具体信息的答案。例如当人问”你喜欢电影吗”的时候你可以回答“我不知道/这很难说”，但一个更令人满意的回答可能是”当然，我特别喜欢科幻电影”。
我们来看一下Meena论文里的结果，因为他们为了比较，在相同的评测体系下把DialoGPT也测了。在Meena论文的评价指标下Meena的水平相当高，人类的SSA大概是0.82，Meena是0.78，已经相当接近。其他的对手在Meena面前基本抬不起头来，DialoGPT 0.51，Cleverbot 0.44。他们还评估了小冰和Mitsuku这两个著名的模块化机器人（非端到端的神经对话模型），SSA分别是0.36和0.56。文中还特别提到小冰的Specificity很差，只有0.19，这和我的使用感受是一致的。小冰感觉总是在扯淡，很难说出有意义的东西来。
SSA体系虽好，但完全依赖人工评价。Meena论文还评估了perplexity和SSA的相关度。结果如上图所示，横坐标是困惑度，纵坐标是SSA，Perplexity和SSA呈现非常明显的负相关关系，相关度大概是0.94。这就是说在训练模型的时候只需要评估混乱度基本就能知道最终的效果。这个结论我觉得非常重要，它无疑给广大神经对话模型研究人员带来了一个福音，毕竟人工评价实在太麻烦也太贵了。这下好了，以后大家奔着降低perplexity去就好了。
对话样例 在这一小节再放几张对话小样，从图中可以看出，这些模型的表现都很好，回答不仅流畅还符合一些常识。当然，我们在前面的文章里也讲过，这种隐含知识不太好控制。但只要语料够优秀，效果看起来是非常棒的。
后记 DialoGPT去年年底就发布了，而且微软大方地提供了预训练好的模型，他们也成为了这个领域当时的SOTA，但没想到这么快就被谷歌超越了。今年初看到谷歌连续放出Meena和REALM的时候还是蛮激动的，一个在类人闲聊上获得突破，另一个大幅提高了外挂知识库的利用水平。Twitter上针对这两篇论文也充满了乐观的论调，甚至有人预测2021年我们就能看到AGI了。这两年的技术发展确实让我们对文本数据的利用水平有了质的飞越，虽然不知道AGI如何，中文房间我感觉很快就会造好了。 另外也感慨Quoc V. Le带领的团队最近势头真猛。谷歌海量算力给了他们无限的空间，已经有在CV（例如去年的EfficientNet已经红遍Kaggle社区了）、NLP等领域遍地开花之势。如果中文房间真的盖好了，门牌上应该写的是1600 Amphitheatre Parkway, Mountain View, CA, U.S.吧。
参考阅读 DialoGPT： http://arxiv....</p>
  </div>
  <footer class="entry-footer"><span title='2020-03-02 00:00:00 +0000 UTC'>March 2, 2020</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 能跟你聊DOTA的神经对话模型：Meena&amp;DialoGPT" href="https://www.yuanhao.site/post/2020-03-02-meena/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="https://www.yuanhao.site/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/page/4/">
      «&nbsp;Prev&nbsp;4/5
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://www.yuanhao.site">多头注意力</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
